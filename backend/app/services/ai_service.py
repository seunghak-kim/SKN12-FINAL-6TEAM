import os
from typing import Dict, Any, Optional, Tuple
from uuid import UUID
from sqlalchemy.orm import Session
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from ..models.chat import ChatSession, ChatMessage
from .prompt_manager import PersonaPromptManager
from pydantic import SecretStr
from dotenv import load_dotenv
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from prompt_chaining import ChainedPromptManager

load_dotenv()
# OPENAPIKEY ìƒì„± 
api_key_str = os.getenv("OPENAI_API_KEY")
OPENAI_API_KEY = SecretStr(api_key_str) if api_key_str is not None else None

class AIService:
    """OpenAI API ì—°ë™ ì„œë¹„ìŠ¤"""
    
    def __init__(self, db: Session):
        self.db = db
        
        # OpenAI API í‚¤ê°€ ìˆìœ¼ë©´ 
        if OPENAI_API_KEY and str(OPENAI_API_KEY) != "None":
            self.llm = ChatOpenAI(model="gpt-4o", api_key=OPENAI_API_KEY, 
                                    temperature=0.9, max_tokens=1000)
        else: # OpenAI APIí‚¤ê°€ ì—†ì„ ê²½ìš° ì—ëŸ¬ ë°œìƒ 
            raise ValueError("OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜(OPENAI_API_KEY)ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        
        # í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.prompt_manager = PersonaPromptManager()
        self.chained_prompt_manager = ChainedPromptManager()
    
    def get_persona_prompt(self, persona_type: str = "ë‚´ë©´í˜•", **context) -> str:
        """í˜ë¥´ì†Œë‚˜ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return self.prompt_manager.get_persona_prompt(persona_type, **context)
    
    def _process_drawing_analysis_for_querock(self, analysis_result: Dict[str, Any]) -> str:
        """ì¾Œë½ì´ íŠ¹ì„±ì— ë§ê²Œ ê·¸ë¦¼ê²€ì‚¬ ê²°ê³¼ë¥¼ ì²˜ë¦¬"""
        if not analysis_result.get('has_result'):
            return "ì•„ì§ ê·¸ë¦¼ê²€ì‚¬ë¥¼ í•´ë³´ì§€ ì•Šìœ¼ì…¨êµ°ìš”! í˜¹ì‹œ ê´€ì‹¬ì´ ìˆìœ¼ì‹œë‹¤ë©´, ë‹¹ì‹ ì˜ ë‚´ë©´ì„¸ê³„ë¥¼ ë” ê¹Šì´ íƒí—˜í•´ë³¼ ìˆ˜ ìˆëŠ” ì¬ë¯¸ìˆëŠ” ë°©ë²•ì´ ìˆì–´ìš”."
        
        scores = analysis_result.get('personality_scores', {})
        querock_score = scores.get('ì¾Œë½ì´', 0.0)
        
        # ì ìˆ˜ë¥¼ 100ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì˜¬ë°”ë¥¸ í¼ì„¼íŠ¸ í‘œì‹œ
        querock_percentage = querock_score / 100
        
        if querock_score > 0.7:
            return f"ë‹¹ì‹ ì˜ ê·¸ë¦¼ì—ì„œ ë³´ì´ëŠ” ì°½ì˜ì ì¸ ì—ë„ˆì§€ê°€ ì •ë§ ì¸ìƒì ì´ì—ìš”! ì¾Œë½ì´ ì ìˆ˜ê°€ {querock_percentage:.2%}ë¡œ ë†’ê²Œ ë‚˜ì˜¨ ê±¸ ë³´ë‹ˆ, ìƒˆë¡œìš´ ê²½í—˜ì„ ì¶”êµ¬í•˜ëŠ” ëª¨í—˜ê°€ì˜ ë§ˆìŒì´ ê°•í•˜ì‹œêµ°ìš”! ë” ìì„¸í•œ ë¶„ì„ì´ ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ ë§ˆì´í˜ì´ì§€ì—ì„œ ê²€ì‚¬ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”."
        elif querock_score > 0.4:
            return f"ê·¸ë¦¼ì—ì„œ ë‹¤ì–‘í•œ ìƒ‰ì±„ì™€ í˜•íƒœë¥¼ ì‚¬ìš©í•˜ì‹  ê±¸ ë³´ë‹ˆ, ìƒˆë¡œìš´ ê²½í—˜ì„ ì¶”êµ¬í•˜ì‹œëŠ” ë¶„ì´ì‹œêµ°ìš”! ì¾Œë½ì´ ì ìˆ˜ê°€ {querock_percentage:.2%}ë¡œ ë‚˜íƒ€ë‚¬ì–´ìš”. ë” ìì„¸í•œ ë¶„ì„ì´ ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ ë§ˆì´í˜ì´ì§€ì—ì„œ ê²€ì‚¬ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”."
        else:
            return f"ê·¸ë¦¼ì—ì„œ ë³´ì´ëŠ” ë‹¤ë¥¸ ì„±í–¥ë“¤ë„ ìˆì§€ë§Œ, ê·¸ ì´ë©´ì—ëŠ” ìƒˆë¡œìš´ ê²½í—˜ì„ ì¶”êµ¬í•˜ëŠ” ëª¨í—˜ê°€ì˜ ë§ˆìŒì´ ìˆ¨ì–´ìˆì„ì§€ë„ ëª°ë¼ìš”. ì¾Œë½ì´ ì ìˆ˜ëŠ” {querock_percentage:.2%}ë¡œ ë‚˜íƒ€ë‚¬ì–´ìš”. ë” ìì„¸í•œ ë¶„ì„ì´ ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ ë§ˆì´í˜ì´ì§€ì—ì„œ ê²€ì‚¬ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”."
    
    def _process_drawing_analysis_for_persona(self, analysis_result: Dict[str, Any], persona_type: str) -> str:
        """í˜ë¥´ì†Œë‚˜ë³„ë¡œ ê·¸ë¦¼ê²€ì‚¬ ê²°ê³¼ë¥¼ ì²˜ë¦¬"""
        if not analysis_result.get('has_result'):
            return "ì•„ì§ ê·¸ë¦¼ê²€ì‚¬ë¥¼ í•´ë³´ì§€ ì•Šìœ¼ì…¨êµ°ìš”! í˜¹ì‹œ ê´€ì‹¬ì´ ìˆìœ¼ì‹œë‹¤ë©´, ë‹¹ì‹ ì˜ ë‚´ë©´ì„¸ê³„ë¥¼ ë” ê¹Šì´ íƒí—˜í•´ë³¼ ìˆ˜ ìˆëŠ” ì¬ë¯¸ìˆëŠ” ë°©ë²•ì´ ìˆì–´ìš”."
        
        scores = analysis_result.get('personality_scores', {})
        
        # í˜ë¥´ì†Œë‚˜ë³„ íŠ¹ì„±ì— ë§ëŠ” í•´ì„
        persona_mapping = {
            "ì¾Œë½í˜•": ("ì¾Œë½ì´", "ì°½ì˜ì ì´ê³  ìƒˆë¡œìš´ ê²½í—˜ì„ ì¶”êµ¬í•˜ëŠ”"),
            "ë‚´ë©´í˜•": ("ë‚´ë©´ì´", "ê¹Šì´ ìˆëŠ” ì‚¬ê³ ì™€ ë‚´ì  ì„±ì¥ì„ ì¤‘ì‹œí•˜ëŠ”"),
            "ì¶”ì§„í˜•": ("ì¶”ì§„ì´", "ëª©í‘œ ì§€í–¥ì ì´ê³  ì‹¤í–‰ë ¥ì´ ë›°ì–´ë‚œ"),
            "ê´€ê³„í˜•": ("ê´€ê³„ì´", "ì‚¬ëŒë“¤ê³¼ì˜ ê´€ê³„ë¥¼ ì¤‘ì‹œí•˜ëŠ”"),
            "ì•ˆì •í˜•": ("ì•ˆì •ì´", "ì•ˆì •ê°ê³¼ ê· í˜•ì„ ì¶”êµ¬í•˜ëŠ”")
        }
        
        target_persona, description = persona_mapping.get(persona_type, ("ì¾Œë½ì´", "ì°½ì˜ì ì¸"))
        target_score = scores.get(target_persona, 0.0)
        
        # ì ìˆ˜ë¥¼ 100ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì˜¬ë°”ë¥¸ í¼ì„¼íŠ¸ í‘œì‹œ
        target_percentage = target_score / 100
        
        if target_score > 0.7:
            return f"ë‹¹ì‹ ì˜ ê·¸ë¦¼ì—ì„œ ë³´ì´ëŠ” {description} íŠ¹ì„±ì´ ì •ë§ ì¸ìƒì ì´ì—ìš”! {target_persona} ì ìˆ˜ê°€ {target_percentage:.2%}ë¡œ ë†’ê²Œ ë‚˜ì˜¨ ê±¸ ë³´ë‹ˆ, ì´ëŸ° ì„±í–¥ì´ ê°•í•˜ì‹œêµ°ìš”! ë” ìì„¸í•œ ë¶„ì„ì´ ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ ë§ˆì´í˜ì´ì§€ì—ì„œ ê²€ì‚¬ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”."
        elif target_score > 0.4:
            return f"ê·¸ë¦¼ì—ì„œ {description} ë©´ëª¨ê°€ ë³´ì´ì‹œë„¤ìš”! {target_persona} ì ìˆ˜ê°€ {target_percentage:.2%}ë¡œ ë‚˜íƒ€ë‚¬ì–´ìš”. ë” ìì„¸í•œ ë¶„ì„ì´ ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ ë§ˆì´í˜ì´ì§€ì—ì„œ ê²€ì‚¬ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”."
        else:
            return f"ê·¸ë¦¼ì—ì„œ ë³´ì´ëŠ” ë‹¤ë¥¸ ì„±í–¥ë“¤ë„ ìˆì§€ë§Œ, ê·¸ ì´ë©´ì—ëŠ” {description} ë§ˆìŒì´ ìˆ¨ì–´ìˆì„ì§€ë„ ëª°ë¼ìš”. {target_persona} ì ìˆ˜ëŠ” {target_percentage:.2%}ë¡œ ë‚˜íƒ€ë‚¬ì–´ìš”. ë” ìì„¸í•œ ë¶„ì„ì´ ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ ë§ˆì´í˜ì´ì§€ì—ì„œ ê²€ì‚¬ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”."
    
    def _generate_drawing_context(self, analysis_result: Dict[str, Any], persona_type: str) -> str:
        """ê·¸ë¦¼ê²€ì‚¬ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not analysis_result.get('has_result'):
            return """
[ê·¸ë¦¼ê²€ì‚¬ ì •ë³´]
ì•„ì§ ê·¸ë¦¼ê²€ì‚¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš©ìê°€ ê·¸ë¦¼ê²€ì‚¬ì— ê´€ì‹¬ì„ ë³´ì´ë©´ ì•ˆë‚´í•´ì£¼ì„¸ìš”.
"""
        
        scores = analysis_result.get('personality_scores', {})
        summary = analysis_result.get('summary', '')
        
        # í˜ë¥´ì†Œë‚˜ë³„ íŠ¹í™”ëœ í•´ì„
        if persona_type == "ì¾Œë½í˜•":
            interpretation = self._process_drawing_analysis_for_querock(analysis_result)
        else:
            interpretation = self._process_drawing_analysis_for_persona(analysis_result, persona_type)
        
        return f"""
[ê·¸ë¦¼ê²€ì‚¬ ë¶„ì„ ê²°ê³¼]
{interpretation}

[ìƒì„¸ ë¶„ì„ ì •ë³´]
- ë¶„ì„ ìš”ì•½: {summary}
- ì„±ê²© ì ìˆ˜: {', '.join([f'{k}: {(v/100):.2%}' for k, v in scores.items() if v > 0])}
- ì£¼ìš” íŠ¹ì„±: {max(scores.items(), key=lambda x: x[1])[0] if scores else 'ë¶„ì„ ì¤‘'}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì‹¬ë¦¬ ìƒíƒœì™€ ì„±í–¥ì„ ê³ ë ¤í•œ ê°œì¸í™”ëœ ìƒë‹´ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
    
    def _detect_character_reference(self, user_message: str) -> Optional[str]:
        """ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ë‹¤ë¥¸ ìºë¦­í„°ì— ëŒ€í•œ ì–¸ê¸‰ì„ ê°ì§€"""
        character_names = self.prompt_manager.get_all_character_names()
        
        for character in character_names:
            if character in user_message:
                return character
        return None
    
    def _get_character_context(self, current_persona: str, referenced_character: str) -> str:
        """ì°¸ì¡°ëœ ìºë¦­í„°ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ìƒì„±"""
        character_info = self.prompt_manager.get_character_info(current_persona, referenced_character)
        
        if character_info:
            return f"""
## ìºë¦­í„° ìƒí˜¸ì‘ìš© ì»¨í…ìŠ¤íŠ¸

ì‚¬ìš©ìê°€ '{referenced_character}'ì— ëŒ€í•´ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

{character_info}

**ë‹µë³€ ê°€ì´ë“œë¼ì¸:**
- {referenced_character}ì˜ íŠ¹ì„±ê³¼ ë§íˆ¬ë¥¼ ì •í™•íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”
- í˜„ì¬ ë‹¹ì‹ ({self.prompt_manager.persona_names.get(current_persona, '')})ì˜ ê´€ì ì—ì„œ {referenced_character}ì„ ì–´ë–»ê²Œ ë³´ëŠ”ì§€ í¬í•¨í•´ì£¼ì„¸ìš”
- {referenced_character}ê°€ ê·¸ëŸ° íŠ¹ì„±ì„ ê°€ì§€ê²Œ ëœ ì´ìœ ë‚˜ ë°°ê²½ì„ ì—ë‹ˆì–´ê·¸ë¨ ê´€ì ì—ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”
- ë”°ëœ»í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”
"""
        return ""
    
    def process_message(self, session_id: UUID, user_message: str, persona_type: str = "ë‚´ë©´í˜•", **context) -> str:
        """2ë‹¨ê³„ ì²´ì´ë‹ìœ¼ë¡œ í˜ë¥´ì†Œë‚˜ ì±—ë´‡ ë©”ì‹œì§€ ì²˜ë¦¬"""
        try:
            # ì„¸ì…˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            session = self.db.query(ChatSession).filter(ChatSession.chat_sessions_id == session_id).first()
            if not session:
                raise ValueError(f"ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {session_id}")
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
            messages = (
                self.db.query(ChatMessage)
                .filter(ChatMessage.session_id == session_id)
                .order_by(ChatMessage.created_at)
                .all()
            )
            
            # íˆìŠ¤í† ë¦¬ ê´€ë¦¬: ë©”ì‹œì§€ê°€ 10ê°œ ì´ìƒì´ë©´ ìš”ì•½ ì—…ë°ì´íŠ¸
            if len(messages) >= 10:
                session = self._manage_conversation_history(session, messages)
            
            # ì»¨í…ìŠ¤íŠ¸ì— user_nickname ì¶”ê°€ (contextì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
            user_nickname = context.get('user_nickname', 'ì‚¬ìš©ì')
            context_with_nickname = {**context, 'user_nickname': user_nickname}
            
            # ìºë¦­í„° ê°„ ìƒí˜¸ì‘ìš© ê°ì§€
            referenced_character = self._detect_character_reference(user_message)
            if referenced_character:
                character_context = self._get_character_context(persona_type, referenced_character)
                context_with_nickname['character_interaction'] = character_context
            
            # 1ë‹¨ê³„: ê³µí†µ ë‹µë³€ ìƒì„± (ì„¸ì…˜ ì •ë³´ í¬í•¨)
            common_response, tokens_step1 = self._generate_common_response(session, messages, user_message, **context_with_nickname)
            
            # 2ë‹¨ê³„: í˜ë¥´ì†Œë‚˜ë³„ ë³€í™˜ (ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬)
            persona_response, tokens_step2 = self._transform_to_persona(common_response, persona_type, user_message, **context_with_nickname)
            
            # ì´ í† í° ì‚¬ìš©ëŸ‰ ì¶œë ¥
            total_tokens = {
                'step1_input': tokens_step1['input'],
                'step1_output': tokens_step1['output'], 
                'step2_input': tokens_step2['input'],
                'step2_output': tokens_step2['output'],
                'total_input': tokens_step1['input'] + tokens_step2['input'],
                'total_output': tokens_step1['output'] + tokens_step2['output'],
                'grand_total': tokens_step1['input'] + tokens_step1['output'] + tokens_step2['input'] + tokens_step2['output']
            }
            
            print(f"ğŸ”— 2ë‹¨ê³„ ì²´ì´ë‹ í† í° ì‚¬ìš©ëŸ‰:")
            print(f"   1ë‹¨ê³„(ê³µí†µë‹µë³€): ì…ë ¥ {total_tokens['step1_input']}, ì¶œë ¥ {total_tokens['step1_output']}")
            print(f"   2ë‹¨ê³„(í˜ë¥´ì†Œë‚˜): ì…ë ¥ {total_tokens['step2_input']}, ì¶œë ¥ {total_tokens['step2_output']}")
            print(f"   ì´í•©: ì…ë ¥ {total_tokens['total_input']}, ì¶œë ¥ {total_tokens['total_output']}, ì „ì²´ {total_tokens['grand_total']}")
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
            user_msg = ChatMessage(
                session_id=session_id,
                sender_type="user",
                content=user_message
            )
            self.db.add(user_msg)
            self.db.flush()
            
            # AI ì‘ë‹µ ë©”ì‹œì§€ ì €ì¥
            assistant_msg = ChatMessage(
                session_id=session_id,
                sender_type="assistant",
                content=persona_response
            )
            self.db.add(assistant_msg)
            
            # ì„¸ì…˜ì˜ updated_at ì—…ë°ì´íŠ¸ (ê°€ì¥ ìµœê·¼ ëŒ€í™” ì‹œê°„ ë°˜ì˜)
            from sqlalchemy.sql import func
            session.updated_at = func.now()
            self.db.add(session)
            
            self.db.commit()
            
            return persona_response
            
        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì‘ë‹µ
            error_response = "ë¯¸ì•ˆí•´... ì§€ê¸ˆì€ ë§ˆìŒì´ ì¢€ ë³µì¡í•´ì„œ ì œëŒ€ë¡œ ë‹µë³€ì„ ì£¼ê¸° ì–´ë ¤ì›Œ. ì ì‹œ í›„ì— ë‹¤ì‹œ ì´ì•¼ê¸°í•´ì¤„ ìˆ˜ ìˆì„ê¹Œ..?"
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ëŠ” ì €ì¥
            try:
                user_msg = ChatMessage(
                    session_id=session_id,
                    sender_type="user",
                    content=user_message
                )
                self.db.add(user_msg)
                self.db.commit()
            except:
                pass
            
            return error_response
    
    def _generate_common_response(self, session: ChatSession, messages: list, user_message: str, **context) -> Tuple[str, Dict[str, int]]:
        """1ë‹¨ê³„: ê³µí†µ ê·œì¹™ìœ¼ë¡œ ê¸°ë³¸ ë‹µë³€ ìƒì„±"""
        try:
            # ê³µí†µ ê·œì¹™ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
            common_rules = self.chained_prompt_manager.load_common_rules()
            
            # ì‚¬ìš©ì ë‹‰ë„¤ì„ ê°€ì ¸ì˜¤ê¸°
            user_nickname = context.get('user_nickname', 'ì‚¬ìš©ì')
            
            # ê·¸ë¦¼ê²€ì‚¬ ë¶„ì„ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            user_analysis_context = ""
            if context.get('user_analysis_result'):
                analysis_result = context['user_analysis_result']
                # ìƒˆë¡œìš´ ìƒì„¸ ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬
                if isinstance(analysis_result, dict) and 'has_result' in analysis_result:
                    # ìƒˆë¡œìš´ í˜•ì‹ì˜ ë¶„ì„ ê²°ê³¼
                    persona_type_for_analysis = context.get('persona_type_for_analysis', context.get('persona_type', 'ë‚´ë©´í˜•'))
                    user_analysis_context = self._generate_drawing_context(analysis_result, persona_type_for_analysis)
                else:
                    # ê¸°ì¡´ í˜•ì‹ì˜ ë¶„ì„ ê²°ê³¼ (í˜¸í™˜ì„± ìœ ì§€)
                    user_analysis_context = f"""

[ì‚¬ìš©ì ê·¸ë¦¼ê²€ì‚¬ ë¶„ì„ ì •ë³´]
ì´ ì‚¬ìš©ìì˜ ê·¸ë¦¼ê²€ì‚¬ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ë” ê°œì¸í™”ëœ ìƒë‹´ì„ ì œê³µí•´ì£¼ì„¸ìš”:
- ì£¼ìš” ì‹¬ë¦¬ íŠ¹ì„±: {analysis_result.get('result_text', 'ë¶„ì„ ì •ë³´ ì—†ìŒ')}
- ë¶„ì„ ìš”ì•½: {analysis_result.get('raw_text', '')[:200]}...

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì‹¬ë¦¬ ìƒíƒœì™€ ì„±í–¥ì„ ê³ ë ¤í•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”."""
            
            # ëŒ€í™” ìš”ì•½ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            conversation_context = ""
            if session.conversation_summary:
                conversation_context = f"""

[ê³¼ê±° ëŒ€í™” ìš”ì•½]
{session.conversation_summary}

ìœ„ ìš”ì•½ì€ ì´ì „ ëŒ€í™”ì˜ í•µì‹¬ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ë¥¼ ì°¸ê³ í•˜ì—¬ ëŒ€í™”ì˜ ì—°ì†ì„±ì„ ìœ ì§€í•´ì£¼ì„¸ìš”."""
            
            # ìºë¦­í„° ìƒí˜¸ì‘ìš© ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            character_interaction_context = ""
            if 'character_interaction' in context and context['character_interaction']:
                character_interaction_context = f"""

{context['character_interaction']}

ìœ„ ìºë¦­í„° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."""
            
            # LLMì— ë³´ë‚¼ ë©”ì‹œì§€ êµ¬ì„±
            llm_messages = []
            llm_messages.append(SystemMessage(content=f"""# ê±°ë¶ì´ìƒë‹´ì†Œ AI ìƒë‹´ì‚¬ - ê³µí†µ ë‹µë³€ ìƒì„±

{common_rules}{user_analysis_context}{conversation_context}{character_interaction_context}

## í˜¸ì¹­ ê·œì¹™
ë‹µë³€í•  ë•Œ ë‹¤ìŒ í˜¸ì¹­ ê·œì¹™ì„ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:
- 'ì‚¬ìš©ì', 'ë„ˆ', 'ë‹¹ì‹ ', 'ê·€í•˜' ëŒ€ì‹  '{user_nickname}' ì‚¬ìš©

ìœ„ ê·œì¹™ì— ë”°ë¼ ì‚¬ìš©ìì˜ ë©”ì‹œì§€ì— ëŒ€í•´ ê¸°ë³¸ì ì´ê³  ì¤‘ë¦½ì ì¸ ìƒë‹´ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
ì´ ë‹µë³€ì€ ë‚˜ì¤‘ì— ê° í˜ë¥´ì†Œë‚˜ì˜ íŠ¹ì„±ì— ë§ê²Œ ë³€í™˜ë  ì˜ˆì •ì…ë‹ˆë‹¤."""))
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 8ê°œë§Œ)
            recent_messages = messages[-8:] if len(messages) > 8 else messages
            for msg in recent_messages:
                if msg.sender_type == "user":
                    llm_messages.append(HumanMessage(content=msg.content))
                else:
                    llm_messages.append(AIMessage(content=msg.content))
            
            # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            llm_messages.append(HumanMessage(content=user_message))
            
            # OpenAI API í˜¸ì¶œ
            response = self.llm.invoke(llm_messages)
            common_response = response.content
            
            # í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°
            tokens = self._calculate_tokens(llm_messages, common_response, response)
            
            return common_response, tokens
            
        except Exception as e:
            print(f"ê³µí†µ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            fallback_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì§€ê¸ˆ ë‹µë³€ì„ ìƒì„±í•˜ëŠ”ë° ì–´ë ¤ì›€ì´ ìˆì–´ìš”. ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ì£¼ì‹œê² ì–´ìš”?"
            return fallback_response, {'input': 0, 'output': 0}
    
    def _transform_to_persona(self, common_response: str, persona_type: str, user_message: str, **context) -> Tuple[str, Dict[str, int]]:
        """2ë‹¨ê³„: ê³µí†µ ë‹µë³€ì„ í˜ë¥´ì†Œë‚˜ íŠ¹ì„±ì— ë§ê²Œ ë³€í™˜"""
        try:
            # í˜ë¥´ì†Œë‚˜ ë§¤í•‘
            persona_mapping = {
                "ë‚´ë©´í˜•": "nemyeon",
                "ì¶”ì§„í˜•": "chujin", 
                "ê´€ê³„í˜•": "gwangye",
                "ì•ˆì •í˜•": "anjeong",
                "ì¾Œë½í˜•": "querock"
            }
            
            persona_key = persona_mapping.get(persona_type, "nemyeon")
            persona_prompt = self.chained_prompt_manager.load_persona_prompt(persona_key)
            
            # ê³µí†µ ê·œì¹™ë„ í•¨ê»˜ ë¡œë“œ
            common_rules = self.chained_prompt_manager.load_common_rules()
            
            # ê·¸ë¦¼ê²€ì‚¬ ë¶„ì„ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            user_analysis_context = ""
            if context.get('user_analysis_result'):
                analysis_result = context['user_analysis_result']
                user_analysis_context = self._generate_drawing_context(analysis_result, persona_type)
            
            # ì‚¬ìš©ì ë‹‰ë„¤ì„ ê°€ì ¸ì˜¤ê¸°
            user_nickname = context.get('user_nickname', 'ì‚¬ìš©ì')
            
            # ë³€í™˜ìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            transform_prompt = f"""# í˜ë¥´ì†Œë‚˜ ë³€í™˜ ì‹œìŠ¤í…œ

{common_rules}

ë‹¤ìŒì€ ë‹¹ì‹ ì˜ í˜ë¥´ì†Œë‚˜ íŠ¹ì„±ì…ë‹ˆë‹¤:

{persona_prompt}{user_analysis_context}

## í˜¸ì¹­ ê·œì¹™
ë‹µë³€í•  ë•Œ ë‹¤ìŒ í˜¸ì¹­ ê·œì¹™ì„ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:
- 'ì‚¬ìš©ì', 'ë„ˆ', 'ë‹¹ì‹ ', 'ê·€í•˜' ëŒ€ì‹  '{user_nickname}ë‹˜' ì‚¬ìš©

## ë³€í™˜ ì‘ì—…
ìœ„ì˜ ê³µí†µ ê·œì¹™, í˜ë¥´ì†Œë‚˜ íŠ¹ì„±, ê·¸ë¦¼ë¶„ì„ ê²°ê³¼ë¥¼ ëª¨ë‘ ë°˜ì˜í•˜ì—¬ ë‹¤ìŒ ê¸°ë³¸ ë‹µë³€ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë³€í™˜í•´ì£¼ì„¸ìš”:

**ê¸°ë³¸ ë‹µë³€:**
{common_response}

**ì‚¬ìš©ì ë©”ì‹œì§€:**
{user_message}

í˜ë¥´ì†Œë‚˜ì˜ ë§íˆ¬, ì„±ê²©, ìƒë‹´ ìŠ¤íƒ€ì¼ì„ ë°˜ì˜í•˜ê³ , ì‚¬ìš©ìì˜ ì‹¬ë¦¬ íŠ¹ì„±ë„ ê³ ë ¤í•˜ì—¬ ë‹µë³€ì„ ë³€í™˜í•´ì£¼ì„¸ìš”. 
ë‹µë³€ì˜ í•µì‹¬ ë‚´ìš©ê³¼ ì˜ë„ëŠ” ìœ ì§€í•˜ë˜, í‘œí˜„ ë°©ì‹ì„ í˜ë¥´ì†Œë‚˜ì™€ ì‚¬ìš©ìì—ê²Œ ë§ê²Œ ì¡°ì •í•´ì£¼ì„¸ìš”."""
            
            llm_messages = [
                SystemMessage(content=transform_prompt),
                HumanMessage(content="ìœ„ ì§€ì¹¨ì— ë”°ë¼ ë‹µë³€ì„ ë³€í™˜í•´ì£¼ì„¸ìš”.")
            ]
            
            # OpenAI API í˜¸ì¶œ
            response = self.llm.invoke(llm_messages)
            persona_response = response.content
            
            # í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°
            tokens = self._calculate_tokens(llm_messages, persona_response, response)
            
            return persona_response, tokens
            
        except Exception as e:
            print(f"í˜ë¥´ì†Œë‚˜ ë³€í™˜ ì˜¤ë¥˜: {e}")
            # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê³µí†µ ë‹µë³€ ë°˜í™˜
            return common_response, {'input': 0, 'output': 0}
    
    def _calculate_tokens(self, llm_messages: list, response_content: str, response_obj) -> Dict[str, int]:
        """í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°"""
        if hasattr(response_obj, 'response_metadata') and 'token_usage' in response_obj.response_metadata:
            token_usage = response_obj.response_metadata['token_usage']
            return {
                'input': token_usage.get('prompt_tokens', 0),
                'output': token_usage.get('completion_tokens', 0)
            }
        else:
            # ëŒ€ëµì ì¸ í† í° ê³„ì‚° (1í† í° â‰ˆ 4ê¸€ì)
            all_text = " ".join([msg.content for msg in llm_messages if hasattr(msg, 'content')])
            estimated_input = len(all_text) // 4
            estimated_output = len(response_content) // 4
            return {
                'input': estimated_input,
                'output': estimated_output
            }

    def get_initial_greeting(self, persona_type: str = "ë‚´ë©´í˜•", user_analysis_result: dict = None) -> str:
        """í˜ë¥´ì†Œë‚˜ë³„ ì´ˆê¸° ì¸ì‚¬ ë©”ì‹œì§€ ë°˜í™˜ (ê·¸ë¦¼ ë¶„ì„ ê²°ê³¼ ë°˜ì˜)"""
        
        # ê·¸ë¦¼ ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ GPT-4oë¡œ ê°œì¸í™”ëœ ì¸ì‚¬ ìƒì„±
        if user_analysis_result:
            try:
                return self._generate_personalized_greeting(persona_type, user_analysis_result)
            except Exception as e:
                print(f"ê°œì¸í™”ëœ ì¸ì‚¬ ìƒì„± ì‹¤íŒ¨: {e}")
        
        # ê¸°ë³¸ ì¸ì‚¬ëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
        return ""

    def _generate_personalized_greeting(self, persona_type: str, user_analysis_result, user_nickname: str = "ì‚¬ìš©ì") -> str:
        """DBì—ì„œ ê°€ì ¸ì˜¨ ê·¸ë¦¼ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ GPT-4oê°€ ê°œì¸í™”ëœ ì²« ì¸ì‚¬ ìƒì„±"""
        
        # DB ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if user_analysis_result and hasattr(user_analysis_result, 'summary_text'):
            analysis_text = user_analysis_result.summary_text
            
            # ì„±ê²© ìœ í˜• ì •ë³´ ì¶”ê°€
            persona_mapping = {
                1: "ì¶”ì§„í˜•",
                2: "ë‚´ë©´í˜•", 
                3: "ê´€ê³„í˜•",
                4: "ì¾Œë½í˜•",
                5: "ì•ˆì •í˜•"
            }
            predicted_personality = persona_mapping.get(user_analysis_result.persona_type, "ë‚´ë©´í˜•")
            
            # í™•ë¥  ì •ë³´ ì¶”ê°€
            scores = {
                "ì¶”ì§„í˜•": float(user_analysis_result.dog_scores or 0.0),
                "ë‚´ë©´í˜•": float(user_analysis_result.cat_scores or 0.0),
                "ê´€ê³„í˜•": float(user_analysis_result.rabbit_scores or 0.0),
                "ì¾Œë½í˜•": float(user_analysis_result.bear_scores or 0.0),
                "ì•ˆì •í˜•": float(user_analysis_result.turtle_scores or 0.0)
            }
            
            analysis_summary = f"""
ê·¸ë¦¼ ë¶„ì„ ê²°ê³¼:
- ë¶„ì„ëœ ì„±ê²© ìœ í˜•: {predicted_personality}
- ì‹¬ë¦¬ ë¶„ì„: {analysis_text}
- ì„±ê²© ì ìˆ˜ ë¶„í¬: {scores}
            """.strip()
        else:
            analysis_summary = "ê·¸ë¦¼ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # ê³µí†µ ê·œì¹™ ë¡œë“œ
        common_rules = self.chained_prompt_manager.load_common_rules()
        
        # í˜ë¥´ì†Œë‚˜ ë§¤í•‘ ë° í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        persona_mapping_prompt = {
            "ë‚´ë©´í˜•": "nemyeon",
            "ì¶”ì§„í˜•": "chujin", 
            "ê´€ê³„í˜•": "gwangye",
            "ì•ˆì •í˜•": "anjeong",
            "ì¾Œë½í˜•": "querock"
        }
        
        persona_key = persona_mapping_prompt.get(persona_type, "nemyeon")
        persona_prompt = self.chained_prompt_manager.load_persona_prompt(persona_key)
        
        # GPT-4o í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""# ê±°ë¶ì´ìƒë‹´ì†Œ AI ìƒë‹´ì‚¬ - ê°œì¸í™”ëœ ì²« ì¸ì‚¬ ìƒì„±

{common_rules}

{persona_prompt}

**ì¤‘ìš”í•œ ë§íˆ¬ ê·œì¹™:**
- ì ˆëŒ€ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš” ("ì•ˆë…•í•˜ì„¸ìš”" ê¸ˆì§€)
- ìœ„ì˜ í˜ë¥´ì†Œë‚˜ íŠ¹ì„±ê³¼ ë§íˆ¬ ê·œì¹™ì„ ì •í™•íˆ ë”°ë¼ì£¼ì„¸ìš”

## í˜¸ì¹­ ê·œì¹™
ë‹µë³€í•  ë•Œ ë‹¤ìŒ í˜¸ì¹­ ê·œì¹™ì„ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:
- 'ì‚¬ìš©ì', 'ë„ˆ', 'ë‹¹ì‹ ', 'ê·€í•˜' ëŒ€ì‹  '{user_nickname}ë‹˜' ì‚¬ìš©
- ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ í†¤ìœ¼ë¡œ ëŒ€í™”
- ì˜ˆ: "ë‹¹ì‹ ì´ ì›í•˜ëŠ”..." â†’ "{user_nickname}ë‹˜ì´ ì›í•˜ì‹œëŠ”..."

ì‚¬ìš©ìì˜ ê·¸ë¦¼ê²€ì‚¬ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì¸í™”ëœ ì²« ì¸ì‚¬ ë©”ì‹œì§€ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

**ê·¸ë¦¼ ë¶„ì„ ê²°ê³¼:**
{analysis_summary}

ìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì˜í•˜ë˜, ë¶„ì„ ë‚´ìš©ì„ ì§ì ‘ ì–¸ê¸‰í•˜ì§€ ë§ê³  ì€ì—°ì¤‘ì— ë“œëŸ¬ë‚˜ëŠ” 150ì ì´ë‚´ì˜ ì²« ì¸ì‚¬ ë©”ì‹œì§€ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."""

        # GPT-4o í˜¸ì¶œ
        from langchain.schema import HumanMessage
        
        response = self.llm.invoke([HumanMessage(content=prompt)])
        greeting = response.content.strip()
        
        print(f"[AI] DB ê¸°ë°˜ ê°œì¸í™”ëœ ì¸ì‚¬ ìƒì„±: {greeting}")
        return greeting
    
    def _manage_conversation_history(self, session: ChatSession, messages: list) -> ChatSession:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬: ìŠ¬ë¼ì´ë”© ìœˆë„ìš° + ìš”ì•½ ë°©ì‹"""
        try:
            # ìµœê·¼ 8ê°œ ë©”ì‹œì§€ëŠ” ìœ ì§€í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” ìš”ì•½ì— í¬í•¨
            recent_messages = messages[-8:]
            old_messages = messages[:-8]
            
            if old_messages:
                # ê¸°ì¡´ ìš”ì•½ê³¼ ì˜¤ë˜ëœ ë©”ì‹œì§€ë“¤ì„ í•©ì³ì„œ ìƒˆë¡œìš´ ìš”ì•½ ìƒì„±
                old_summary = session.conversation_summary or ""
                new_summary = self._generate_conversation_summary(old_summary, old_messages)
                
                # ì„¸ì…˜ì— ìš”ì•½ ì—…ë°ì´íŠ¸
                session.conversation_summary = new_summary
                self.db.add(session)
                self.db.flush()
                
                print(f"[íˆìŠ¤í† ë¦¬] ëŒ€í™” ìš”ì•½ ì—…ë°ì´íŠ¸: {len(old_messages)}ê°œ ë©”ì‹œì§€ ìš”ì•½ë¨")
                print(f"[íˆìŠ¤í† ë¦¬] ìš”ì•½ ë‚´ìš©: {new_summary}")
                
        except Exception as e:
            print(f"ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ì˜¤ë¥˜: {e}")
            
        return session
    
    def _generate_conversation_summary(self, existing_summary: str, messages: list) -> str:
        """ê³¼ê±° ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½ ìƒì„±"""
        try:
            # ë©”ì‹œì§€ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            conversation_text = []
            for msg in messages:
                sender = "ì‚¬ìš©ì" if msg.sender_type == "user" else "ìƒë‹´ì‚¬"
                conversation_text.append(f"{sender}: {msg.content}")
            
            conversation_str = "\n".join(conversation_text)
            
            # ìš”ì•½ í”„ë¡¬í”„íŠ¸
            summary_prompt = f"""ë‹¤ìŒì€ ì‹¬ë¦¬ ìƒë‹´ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤. ì´ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ê¸°ì¡´ ìš”ì•½ (ìˆë‹¤ë©´):
{existing_summary}

ìƒˆë¡œìš´ ëŒ€í™” ë‚´ìš©:
{conversation_str}

ìš”êµ¬ì‚¬í•­:
1. ì‚¬ìš©ìì˜ ì£¼ìš” ê³ ë¯¼ê³¼ ê°ì • ìƒíƒœë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½
2. ìƒë‹´ì‚¬ê°€ ì œê³µí•œ ì£¼ìš” ì¡°ì–¸ì´ë‚˜ í†µì°° í¬í•¨  
3. ëŒ€í™”ì˜ íë¦„ê³¼ ì¤‘ìš”í•œ ì „í™˜ì  ê¸°ë¡
4. 200ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±
5. ê¸°ì¡´ ìš”ì•½ì´ ìˆë‹¤ë©´ í†µí•©í•˜ì—¬ ì‘ì„±

ìš”ì•½:"""

            # GPTë¡œ ìš”ì•½ ìƒì„±
            from langchain.schema import HumanMessage
            response = self.llm.invoke([HumanMessage(content=summary_prompt)])
            summary = response.content.strip()
            
            print(f"[ìš”ì•½] ìƒì„±ëœ ëŒ€í™” ìš”ì•½: {summary[:100]}...")
            return summary
            
        except Exception as e:
            print(f"ëŒ€í™” ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ìš”ì•½ ë°˜í™˜
            return existing_summary or "ëŒ€í™” ìš”ì•½ ìƒì„± ì‹¤íŒ¨"
    
    def get_available_personas(self) -> list:
        """ì‚¬ìš© ê°€ëŠ¥í•œ í˜ë¥´ì†Œë‚˜ ëª©ë¡ ë°˜í™˜"""
        return self.prompt_manager.get_available_personas()
import os
from typing import Dict, Any, Optional, Tuple
from uuid import UUID
from sqlalchemy.orm import Session
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from ..models.chat import ChatSession, ChatMessage
from .prompt_manager import PersonaPromptManager
from pydantic import SecretStr
from dotenv import load_dotenv
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from prompt_chaining import ChainedPromptManager

load_dotenv()
# OPENAPIKEY ìƒì„± 
api_key_str = os.getenv("OPENAI_API_KEY")
OPENAI_API_KEY = SecretStr(api_key_str) if api_key_str is not None else None

class AIService:
    """OpenAI API ì—°ë™ ì„œë¹„ìŠ¤"""
    
    def __init__(self, db: Session):
        self.db = db
        
        # OpenAI API í‚¤ê°€ ìˆìœ¼ë©´ 
        if OPENAI_API_KEY and str(OPENAI_API_KEY) != "None":
            self.llm = ChatOpenAI(model="gpt-4o", api_key=OPENAI_API_KEY, 
                                    temperature=0.9, max_tokens=1000)
        else: # OpenAI APIí‚¤ê°€ ì—†ì„ ê²½ìš° ì—ëŸ¬ ë°œìƒ 
            raise ValueError("OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜(OPENAI_API_KEY)ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        
        # í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.prompt_manager = PersonaPromptManager()
        self.chained_prompt_manager = ChainedPromptManager()
    
    def get_persona_prompt(self, persona_type: str = "ë‚´ë©´í˜•", **context) -> str:
        """í˜ë¥´ì†Œë‚˜ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return self.prompt_manager.get_persona_prompt(persona_type, **context)
    
    def process_message(self, session_id: UUID, user_message: str, persona_type: str = "ë‚´ë©´í˜•", **context) -> str:
        """2ë‹¨ê³„ ì²´ì´ë‹ìœ¼ë¡œ í˜ë¥´ì†Œë‚˜ ì±—ë´‡ ë©”ì‹œì§€ ì²˜ë¦¬"""
        try:
            # ì„¸ì…˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            session = self.db.query(ChatSession).filter(ChatSession.chat_sessions_id == session_id).first()
            if not session:
                raise ValueError(f"ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {session_id}")
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
            messages = (
                self.db.query(ChatMessage)
                .filter(ChatMessage.session_id == session_id)
                .order_by(ChatMessage.created_at)
                .all()
            )
            
            # íˆìŠ¤í† ë¦¬ ê´€ë¦¬: ë©”ì‹œì§€ê°€ 10ê°œ ì´ìƒì´ë©´ ìš”ì•½ ì—…ë°ì´íŠ¸
            if len(messages) >= 10:
                session = self._manage_conversation_history(session, messages)
            
            # 1ë‹¨ê³„: ê³µí†µ ë‹µë³€ ìƒì„± (ì„¸ì…˜ ì •ë³´ í¬í•¨)
            common_response, tokens_step1 = self._generate_common_response(session, messages, user_message, **context)
            
            # 2ë‹¨ê³„: í˜ë¥´ì†Œë‚˜ë³„ ë³€í™˜
            persona_response, tokens_step2 = self._transform_to_persona(common_response, persona_type, user_message, **context)
            
            # ì´ í† í° ì‚¬ìš©ëŸ‰ ì¶œë ¥
            total_tokens = {
                'step1_input': tokens_step1['input'],
                'step1_output': tokens_step1['output'], 
                'step2_input': tokens_step2['input'],
                'step2_output': tokens_step2['output'],
                'total_input': tokens_step1['input'] + tokens_step2['input'],
                'total_output': tokens_step1['output'] + tokens_step2['output'],
                'grand_total': tokens_step1['input'] + tokens_step1['output'] + tokens_step2['input'] + tokens_step2['output']
            }
            
            print(f"ğŸ”— 2ë‹¨ê³„ ì²´ì´ë‹ í† í° ì‚¬ìš©ëŸ‰:")
            print(f"   1ë‹¨ê³„(ê³µí†µë‹µë³€): ì…ë ¥ {total_tokens['step1_input']}, ì¶œë ¥ {total_tokens['step1_output']}")
            print(f"   2ë‹¨ê³„(í˜ë¥´ì†Œë‚˜): ì…ë ¥ {total_tokens['step2_input']}, ì¶œë ¥ {total_tokens['step2_output']}")
            print(f"   ì´í•©: ì…ë ¥ {total_tokens['total_input']}, ì¶œë ¥ {total_tokens['total_output']}, ì „ì²´ {total_tokens['grand_total']}")
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
            user_msg = ChatMessage(
                session_id=session_id,
                sender_type="user",
                content=user_message
            )
            self.db.add(user_msg)
            self.db.flush()
            
            # AI ì‘ë‹µ ë©”ì‹œì§€ ì €ì¥
            assistant_msg = ChatMessage(
                session_id=session_id,
                sender_type="assistant",
                content=persona_response
            )
            self.db.add(assistant_msg)
            
            # ì„¸ì…˜ì˜ updated_at ì—…ë°ì´íŠ¸ (ê°€ì¥ ìµœê·¼ ëŒ€í™” ì‹œê°„ ë°˜ì˜)
            from sqlalchemy.sql import func
            session.updated_at = func.now()
            self.db.add(session)
            
            self.db.commit()
            
            return persona_response
            
        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì‘ë‹µ
            error_response = "ë¯¸ì•ˆí•´... ì§€ê¸ˆì€ ë§ˆìŒì´ ì¢€ ë³µì¡í•´ì„œ ì œëŒ€ë¡œ ë‹µë³€ì„ ì£¼ê¸° ì–´ë ¤ì›Œ. ì ì‹œ í›„ì— ë‹¤ì‹œ ì´ì•¼ê¸°í•´ì¤„ ìˆ˜ ìˆì„ê¹Œ..?"
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ëŠ” ì €ì¥
            try:
                user_msg = ChatMessage(
                    session_id=session_id,
                    sender_type="user",
                    content=user_message
                )
                self.db.add(user_msg)
                self.db.commit()
            except:
                pass
            
            return error_response
    
    def _generate_common_response(self, session: ChatSession, messages: list, user_message: str, **context) -> Tuple[str, Dict[str, int]]:
        """1ë‹¨ê³„: ê³µí†µ ê·œì¹™ìœ¼ë¡œ ê¸°ë³¸ ë‹µë³€ ìƒì„±"""
        try:
            # ê³µí†µ ê·œì¹™ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
            common_rules = self.chained_prompt_manager.load_common_rules()
            
            # ê·¸ë¦¼ê²€ì‚¬ ë¶„ì„ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            user_analysis_context = ""
            if context.get('user_analysis_result'):
                analysis_result = context['user_analysis_result']
                user_analysis_context = f"""

[ì‚¬ìš©ì ê·¸ë¦¼ê²€ì‚¬ ë¶„ì„ ì •ë³´]
ì´ ì‚¬ìš©ìì˜ ê·¸ë¦¼ê²€ì‚¬ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ë” ê°œì¸í™”ëœ ìƒë‹´ì„ ì œê³µí•´ì£¼ì„¸ìš”:
- ì£¼ìš” ì‹¬ë¦¬ íŠ¹ì„±: {analysis_result.get('result_text', 'ë¶„ì„ ì •ë³´ ì—†ìŒ')}
- ë¶„ì„ ìš”ì•½: {analysis_result.get('raw_text', '')[:200]}...

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì‹¬ë¦¬ ìƒíƒœì™€ ì„±í–¥ì„ ê³ ë ¤í•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”."""
            
            # ëŒ€í™” ìš”ì•½ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            conversation_context = ""
            if session.conversation_summary:
                conversation_context = f"""

[ê³¼ê±° ëŒ€í™” ìš”ì•½]
{session.conversation_summary}

ìœ„ ìš”ì•½ì€ ì´ì „ ëŒ€í™”ì˜ í•µì‹¬ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ë¥¼ ì°¸ê³ í•˜ì—¬ ëŒ€í™”ì˜ ì—°ì†ì„±ì„ ìœ ì§€í•´ì£¼ì„¸ìš”."""
            
            # LLMì— ë³´ë‚¼ ë©”ì‹œì§€ êµ¬ì„±
            llm_messages = []
            llm_messages.append(SystemMessage(content=f"""# ê±°ë¶ì´ìƒë‹´ì†Œ AI ìƒë‹´ì‚¬ - ê³µí†µ ë‹µë³€ ìƒì„±

{common_rules}{user_analysis_context}{conversation_context}

ìœ„ ê·œì¹™ì— ë”°ë¼ ì‚¬ìš©ìì˜ ë©”ì‹œì§€ì— ëŒ€í•´ ê¸°ë³¸ì ì´ê³  ì¤‘ë¦½ì ì¸ ìƒë‹´ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
ì´ ë‹µë³€ì€ ë‚˜ì¤‘ì— ê° í˜ë¥´ì†Œë‚˜ì˜ íŠ¹ì„±ì— ë§ê²Œ ë³€í™˜ë  ì˜ˆì •ì…ë‹ˆë‹¤."""))
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 8ê°œë§Œ)
            recent_messages = messages[-8:] if len(messages) > 8 else messages
            for msg in recent_messages:
                if msg.sender_type == "user":
                    llm_messages.append(HumanMessage(content=msg.content))
                else:
                    llm_messages.append(AIMessage(content=msg.content))
            
            # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            llm_messages.append(HumanMessage(content=user_message))
            
            # OpenAI API í˜¸ì¶œ
            response = self.llm.invoke(llm_messages)
            common_response = response.content
            
            # í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°
            tokens = self._calculate_tokens(llm_messages, common_response, response)
            
            return common_response, tokens
            
        except Exception as e:
            print(f"ê³µí†µ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            fallback_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì§€ê¸ˆ ë‹µë³€ì„ ìƒì„±í•˜ëŠ”ë° ì–´ë ¤ì›€ì´ ìˆì–´ìš”. ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ì£¼ì‹œê² ì–´ìš”?"
            return fallback_response, {'input': 0, 'output': 0}
    
    def _transform_to_persona(self, common_response: str, persona_type: str, user_message: str, **context) -> Tuple[str, Dict[str, int]]:
        """2ë‹¨ê³„: ê³µí†µ ë‹µë³€ì„ í˜ë¥´ì†Œë‚˜ íŠ¹ì„±ì— ë§ê²Œ ë³€í™˜"""
        try:
            # í˜ë¥´ì†Œë‚˜ ë§¤í•‘
            persona_mapping = {
                "ë‚´ë©´í˜•": "nemyeon",
                "ì¶”ì§„í˜•": "chujin", 
                "ê´€ê³„í˜•": "gwangye",
                "ì•ˆì •í˜•": "anjeong",
                "ì¾Œë½í˜•": "querock"
            }
            
            persona_key = persona_mapping.get(persona_type, "nemyeon")
            persona_prompt = self.chained_prompt_manager.load_persona_prompt(persona_key)
            
            # ë³€í™˜ìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            transform_prompt = f"""# í˜ë¥´ì†Œë‚˜ ë³€í™˜ ì‹œìŠ¤í…œ

ë‹¤ìŒì€ ë‹¹ì‹ ì˜ í˜ë¥´ì†Œë‚˜ íŠ¹ì„±ì…ë‹ˆë‹¤:

{persona_prompt}

## ë³€í™˜ ì‘ì—…
ìœ„ì˜ í˜ë¥´ì†Œë‚˜ íŠ¹ì„±ì— ë§ê²Œ ë‹¤ìŒ ê¸°ë³¸ ë‹µë³€ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë³€í™˜í•´ì£¼ì„¸ìš”:

**ê¸°ë³¸ ë‹µë³€:**
{common_response}

**ì‚¬ìš©ì ë©”ì‹œì§€:**
{user_message}

í˜ë¥´ì†Œë‚˜ì˜ ë§íˆ¬, ì„±ê²©, ìƒë‹´ ìŠ¤íƒ€ì¼ì„ ë°˜ì˜í•˜ì—¬ ë‹µë³€ì„ ë³€í™˜í•´ì£¼ì„¸ìš”. 
ë‹µë³€ì˜ í•µì‹¬ ë‚´ìš©ê³¼ ì˜ë„ëŠ” ìœ ì§€í•˜ë˜, í‘œí˜„ ë°©ì‹ì„ í˜ë¥´ì†Œë‚˜ì— ë§ê²Œ ì¡°ì •í•´ì£¼ì„¸ìš”."""
            
            llm_messages = [
                SystemMessage(content=transform_prompt),
                HumanMessage(content="ìœ„ ì§€ì¹¨ì— ë”°ë¼ ë‹µë³€ì„ ë³€í™˜í•´ì£¼ì„¸ìš”.")
            ]
            
            # OpenAI API í˜¸ì¶œ
            response = self.llm.invoke(llm_messages)
            persona_response = response.content
            
            # í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°
            tokens = self._calculate_tokens(llm_messages, persona_response, response)
            
            return persona_response, tokens
            
        except Exception as e:
            print(f"í˜ë¥´ì†Œë‚˜ ë³€í™˜ ì˜¤ë¥˜: {e}")
            # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê³µí†µ ë‹µë³€ ë°˜í™˜
            return common_response, {'input': 0, 'output': 0}
    
    def _calculate_tokens(self, llm_messages: list, response_content: str, response_obj) -> Dict[str, int]:
        """í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°"""
        if hasattr(response_obj, 'response_metadata') and 'token_usage' in response_obj.response_metadata:
            token_usage = response_obj.response_metadata['token_usage']
            return {
                'input': token_usage.get('prompt_tokens', 0),
                'output': token_usage.get('completion_tokens', 0)
            }
        else:
            # ëŒ€ëµì ì¸ í† í° ê³„ì‚° (1í† í° â‰ˆ 4ê¸€ì)
            all_text = " ".join([msg.content for msg in llm_messages if hasattr(msg, 'content')])
            estimated_input = len(all_text) // 4
            estimated_output = len(response_content) // 4
            return {
                'input': estimated_input,
                'output': estimated_output
            }

    def get_initial_greeting(self, persona_type: str = "ë‚´ë©´í˜•", user_analysis_result: dict = None) -> str:
        """í˜ë¥´ì†Œë‚˜ë³„ ì´ˆê¸° ì¸ì‚¬ ë©”ì‹œì§€ ë°˜í™˜ (ê·¸ë¦¼ ë¶„ì„ ê²°ê³¼ ë°˜ì˜)"""
        
        # ê·¸ë¦¼ ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ GPT-4oë¡œ ê°œì¸í™”ëœ ì¸ì‚¬ ìƒì„±
        if user_analysis_result:
            try:
                return self._generate_personalized_greeting(persona_type, user_analysis_result)
            except Exception as e:
                print(f"ê°œì¸í™”ëœ ì¸ì‚¬ ìƒì„± ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì¸ì‚¬ë¡œ í´ë°±
        
        # ê¸°ë³¸ ì¸ì‚¬ë§ (í´ë°±ìš©)
        base_greetings = {
            "ë‚´ë©´í˜•": "ì•ˆë…•... ë‚˜ëŠ” ë‚´ë©´ì´ì•¼. ë§ë³´ë‹¤ ëŠë‚Œì´ ë¨¼ì €ì¼ ë•Œ, ì¡°ìš©í•œ ë§ˆìŒìœ¼ë¡œ í•¨ê»˜ ìˆì„ ìˆ˜ ìˆë‹¤ë©´ ì¢‹ê² ì–´.",
            "ì¶”ì§„í˜•": "ì €ëŠ” ë‹¹ì‹ ì˜ ê³ ë¯¼ì„ í•¨ê»˜ í•´ê²°í•´ê°ˆ ì¶”ì§„ì´ì—ìš”. ì§€ê¸ˆë¶€í„° ê°€ì¥ ì¤‘ìš”í•œ ì–˜ê¸°ë¥¼ í•´ë³¼ê¹Œìš”?",
            "ê´€ê³„í˜•": "ì €ëŠ” ë‹¹ì‹ ì˜ ê³ ë¯¼ì„ í•¨ê»˜ í•´ê²°í•´ê°ˆ ê´€ê³„ì´ì—ìš”. ì§€ê¸ˆë¶€í„° ë§ˆìŒ ì† ê³ ë¯¼ì„ ì–˜ê¸°í•´ë³¼ê¹Œìš”?",
            "ì•ˆì •í˜•": "ì €ëŠ” ë‹¹ì‹ ì„ ì•ˆì •ì‹œì¼œë“œë¦´ ì•ˆì •ì´ì—ìš”. ì§€ê¸ˆë¶€í„° ë§ˆìŒ ì† ê³ ë¯¼ì„ ì–˜ê¸°í•´ë³¼ê¹Œìš”?",
            "ì¾Œë½í˜•": "ì €ëŠ” ë‹¹ì‹ ì˜ ê³ ë¯¼ì„ í•¨ê»˜ í•´ê²°í•´ê°ˆ ì¾Œë½ì´ì—ìš”. ì§€ê¸ˆë¶€í„° ì¬ë°ŒëŠ” ì–˜ê¸°ë¥¼ í•´ë³¼ê¹Œìš”?"
        }
        
        return base_greetings.get(persona_type, base_greetings["ë‚´ë©´í˜•"])

    def _generate_personalized_greeting(self, persona_type: str, user_analysis_result: dict) -> str:
        """ê·¸ë¦¼ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ GPT-4oê°€ ê°œì¸í™”ëœ ì²« ì¸ì‚¬ ìƒì„±"""
        
        # í˜ë¥´ì†Œë‚˜ë³„ ê¸°ë³¸ ì„±ê²© ì„¤ëª…
        persona_descriptions = {
            "ë‚´ë©´í˜•": "ë‚´ë©´ì´: ê°ì •ê³¼ ì‚¬ê³ ì˜ ê¹Šì€ ë°”ë‹¤ë¥¼ íƒí—˜í•˜ëŠ” ì¡°ë ¥ì. ë¹„ê´€ì  ì„±í–¥ì„ ê°€ì§€ê³  ìˆìœ¼ë©° ê¹Šì´ ìˆëŠ” í†µì°° ì œê³µ",
            "ì¶”ì§„í˜•": "ì¶”ì§„ì´: ëª©í‘œ ë‹¬ì„±ì„ ê°•ë ¥íˆ ì¶”ì§„í•˜ëŠ” ì „ëµê°€. ê²°ê³¼ ì§€í–¥ì ì´ê³  íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ëŠ” ìŠ¤íƒ€ì¼",
            "ê´€ê³„í˜•": "ê´€ê³„ì´: ì‚¬ëŒë“¤ê³¼ì˜ ê´€ê³„ì™€ ì†Œí†µì„ ì¤‘ì‹œí•˜ëŠ” ë”°ëœ»í•œ ìƒë‹´ì",
            "ì•ˆì •í˜•": "ì•ˆì •ì´: ì¤‘ë¦½ê³¼ ì¡°í™”ë¥¼ ì§€í–¥í•˜ëŠ” ì•ˆì „í•œ ëŒ€í™” íŒŒíŠ¸ë„ˆ. ì‹ ì¤‘í•˜ê³  ë¶€ë“œëŸ¬ìš´ ë§íˆ¬",
            "ì¾Œë½í˜•": "ì¾Œë½ì´: ì¦ê±°ì›€ê³¼ í™œë ¥ì„ ì¶”êµ¬í•˜ëŠ” ë°˜ì¯¤ ë¯¸ì¹œ ì—ë„ˆì§€ ë„˜ì¹˜ëŠ” ë™ë°˜ì"
        }
        
        # GPT-4o í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¹ì‹ ì€ {persona_descriptions.get(persona_type, persona_descriptions['ë‚´ë©´í˜•'])}ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ê·¸ë¦¼ê²€ì‚¬ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì¸í™”ëœ ì²« ì¸ì‚¬ ë©”ì‹œì§€ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

**ê·¸ë¦¼ ë¶„ì„ ê²°ê³¼:**
{user_analysis_result}

**ìš”êµ¬ì‚¬í•­:**
1. ìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì˜í•˜ë˜, ë¶„ì„ ë‚´ìš©ì„ ì§ì ‘ ì–¸ê¸‰í•˜ì§€ ë§ê³  ì€ì—°ì¤‘ì— ë“œëŸ¬ë‚˜ë„ë¡ í•˜ì„¸ìš”
2. í•´ë‹¹ í˜ë¥´ì†Œë‚˜ì˜ ë§íˆ¬ì™€ ì„±ê²©ì— ë§ê²Œ ì‘ì„±í•˜ì„¸ìš”
3. ë”°ëœ»í•˜ê³  ê³µê°ì ì¸ í†¤ìœ¼ë¡œ ì²« ì¸ì‚¬ë¥¼ ê±´ë„¤ì„¸ìš”
4. 150ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”
5. ì‚¬ìš©ìê°€ í¸ì•ˆí•˜ê²Œ ëŒ€í™”ë¥¼ ì‹œì‘í•  ìˆ˜ ìˆë„ë¡ ìœ ë„í•˜ì„¸ìš”

ì²« ì¸ì‚¬ ë©”ì‹œì§€:"""

        # GPT-4o í˜¸ì¶œ
        from langchain.schema import HumanMessage
        
        response = self.llm.invoke([HumanMessage(content=prompt)])
        greeting = response.content.strip()
        
        print(f"[AI] ê°œì¸í™”ëœ ì¸ì‚¬ ìƒì„±: {greeting}")
        return greeting
    
    def _manage_conversation_history(self, session: ChatSession, messages: list) -> ChatSession:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬: ìŠ¬ë¼ì´ë”© ìœˆë„ìš° + ìš”ì•½ ë°©ì‹"""
        try:
            # ìµœê·¼ 8ê°œ ë©”ì‹œì§€ëŠ” ìœ ì§€í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” ìš”ì•½ì— í¬í•¨
            recent_messages = messages[-8:]
            old_messages = messages[:-8]
            
            if old_messages:
                # ê¸°ì¡´ ìš”ì•½ê³¼ ì˜¤ë˜ëœ ë©”ì‹œì§€ë“¤ì„ í•©ì³ì„œ ìƒˆë¡œìš´ ìš”ì•½ ìƒì„±
                old_summary = session.conversation_summary or ""
                new_summary = self._generate_conversation_summary(old_summary, old_messages)
                
                # ì„¸ì…˜ì— ìš”ì•½ ì—…ë°ì´íŠ¸
                session.conversation_summary = new_summary
                self.db.add(session)
                self.db.flush()
                
                print(f"[íˆìŠ¤í† ë¦¬] ëŒ€í™” ìš”ì•½ ì—…ë°ì´íŠ¸: {len(old_messages)}ê°œ ë©”ì‹œì§€ ìš”ì•½ë¨")
                
        except Exception as e:
            print(f"ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ì˜¤ë¥˜: {e}")
            
        return session
    
    def _generate_conversation_summary(self, existing_summary: str, messages: list) -> str:
        """ê³¼ê±° ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½ ìƒì„±"""
        try:
            # ë©”ì‹œì§€ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            conversation_text = []
            for msg in messages:
                sender = "ì‚¬ìš©ì" if msg.sender_type == "user" else "ìƒë‹´ì‚¬"
                conversation_text.append(f"{sender}: {msg.content}")
            
            conversation_str = "\n".join(conversation_text)
            
            # ìš”ì•½ í”„ë¡¬í”„íŠ¸
            summary_prompt = f"""ë‹¤ìŒì€ ì‹¬ë¦¬ ìƒë‹´ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤. ì´ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ê¸°ì¡´ ìš”ì•½ (ìˆë‹¤ë©´):
{existing_summary}

ìƒˆë¡œìš´ ëŒ€í™” ë‚´ìš©:
{conversation_str}

ìš”êµ¬ì‚¬í•­:
1. ì‚¬ìš©ìì˜ ì£¼ìš” ê³ ë¯¼ê³¼ ê°ì • ìƒíƒœë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½
2. ìƒë‹´ì‚¬ê°€ ì œê³µí•œ ì£¼ìš” ì¡°ì–¸ì´ë‚˜ í†µì°° í¬í•¨  
3. ëŒ€í™”ì˜ íë¦„ê³¼ ì¤‘ìš”í•œ ì „í™˜ì  ê¸°ë¡
4. 200ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±
5. ê¸°ì¡´ ìš”ì•½ì´ ìˆë‹¤ë©´ í†µí•©í•˜ì—¬ ì‘ì„±

ìš”ì•½:"""

            # GPTë¡œ ìš”ì•½ ìƒì„±
            from langchain.schema import HumanMessage
            response = self.llm.invoke([HumanMessage(content=summary_prompt)])
            summary = response.content.strip()
            
            print(f"[ìš”ì•½] ìƒì„±ëœ ëŒ€í™” ìš”ì•½: {summary[:100]}...")
            return summary
            
        except Exception as e:
            print(f"ëŒ€í™” ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ìš”ì•½ ë°˜í™˜
            return existing_summary or "ëŒ€í™” ìš”ì•½ ìƒì„± ì‹¤íŒ¨"
    
    def get_available_personas(self) -> list:
        """ì‚¬ìš© ê°€ëŠ¥í•œ í˜ë¥´ì†Œë‚˜ ëª©ë¡ ë°˜í™˜"""
        return self.prompt_manager.get_available_personas()
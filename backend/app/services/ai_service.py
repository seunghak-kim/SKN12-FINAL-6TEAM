import os
from typing import Dict, Any, Optional, Tuple
from uuid import UUID
from sqlalchemy.orm import Session
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from ..models.chat import ChatSession, ChatMessage
from .prompt_manager import PersonaPromptManager
from pydantic import SecretStr
from dotenv import load_dotenv
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from prompt_chaining import ChainedPromptManager

load_dotenv()
# OPENAPIKEY ìƒì„± 
api_key_str = os.getenv("OPENAI_API_KEY")
OPENAI_API_KEY = SecretStr(api_key_str) if api_key_str is not None else None

class AIService:
    """OpenAI API ì—°ë™ ì„œë¹„ìŠ¤"""
    
    def __init__(self, db: Session):
        self.db = db
        
        # OpenAI API í‚¤ê°€ ìˆìœ¼ë©´ 
        if OPENAI_API_KEY and str(OPENAI_API_KEY) != "None":
            self.llm = ChatOpenAI(model="gpt-4o", api_key=OPENAI_API_KEY, 
                                    temperature=0.9, max_tokens=1000)
        else: # OpenAI APIí‚¤ê°€ ì—†ì„ ê²½ìš° ì—ëŸ¬ ë°œìƒ 
            raise ValueError("OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜(OPENAI_API_KEY)ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        
        # í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.prompt_manager = PersonaPromptManager()
        self.chained_prompt_manager = ChainedPromptManager()
    
    def get_persona_prompt(self, persona_type: str = "ë‚´ë©´í˜•", **context) -> str:
        """í˜ë¥´ì†Œë‚˜ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return self.prompt_manager.get_persona_prompt(persona_type, **context)
    
    def process_message(self, session_id: UUID, user_message: str, persona_type: str = "ë‚´ë©´í˜•", **context) -> str:
        """2ë‹¨ê³„ ì²´ì´ë‹ìœ¼ë¡œ í˜ë¥´ì†Œë‚˜ ì±—ë´‡ ë©”ì‹œì§€ ì²˜ë¦¬"""
        try:
            # ì„¸ì…˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            session = self.db.query(ChatSession).filter(ChatSession.chat_sessions_id == session_id).first()
            if not session:
                raise ValueError(f"ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {session_id}")
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
            messages = (
                self.db.query(ChatMessage)
                .filter(ChatMessage.session_id == session_id)
                .order_by(ChatMessage.created_at)
                .all()
            )
            
            # 1ë‹¨ê³„: ê³µí†µ ë‹µë³€ ìƒì„±
            common_response, tokens_step1 = self._generate_common_response(messages, user_message, **context)
            
            # 2ë‹¨ê³„: í˜ë¥´ì†Œë‚˜ë³„ ë³€í™˜
            persona_response, tokens_step2 = self._transform_to_persona(common_response, persona_type, user_message, **context)
            
            # ì´ í† í° ì‚¬ìš©ëŸ‰ ì¶œë ¥
            total_tokens = {
                'step1_input': tokens_step1['input'],
                'step1_output': tokens_step1['output'], 
                'step2_input': tokens_step2['input'],
                'step2_output': tokens_step2['output'],
                'total_input': tokens_step1['input'] + tokens_step2['input'],
                'total_output': tokens_step1['output'] + tokens_step2['output'],
                'grand_total': tokens_step1['input'] + tokens_step1['output'] + tokens_step2['input'] + tokens_step2['output']
            }
            
            print(f"ğŸ”— 2ë‹¨ê³„ ì²´ì´ë‹ í† í° ì‚¬ìš©ëŸ‰:")
            print(f"   1ë‹¨ê³„(ê³µí†µë‹µë³€): ì…ë ¥ {total_tokens['step1_input']}, ì¶œë ¥ {total_tokens['step1_output']}")
            print(f"   2ë‹¨ê³„(í˜ë¥´ì†Œë‚˜): ì…ë ¥ {total_tokens['step2_input']}, ì¶œë ¥ {total_tokens['step2_output']}")
            print(f"   ì´í•©: ì…ë ¥ {total_tokens['total_input']}, ì¶œë ¥ {total_tokens['total_output']}, ì „ì²´ {total_tokens['grand_total']}")
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
            user_msg = ChatMessage(
                session_id=session_id,
                sender_type="user",
                content=user_message
            )
            self.db.add(user_msg)
            self.db.flush()
            
            # AI ì‘ë‹µ ë©”ì‹œì§€ ì €ì¥
            assistant_msg = ChatMessage(
                session_id=session_id,
                sender_type="assistant",
                content=persona_response
            )
            self.db.add(assistant_msg)
            
            # ì„¸ì…˜ì˜ updated_at ì—…ë°ì´íŠ¸ (ê°€ì¥ ìµœê·¼ ëŒ€í™” ì‹œê°„ ë°˜ì˜)
            from sqlalchemy.sql import func
            session.updated_at = func.now()
            self.db.add(session)
            
            self.db.commit()
            
            return persona_response
            
        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì‘ë‹µ
            error_response = "ë¯¸ì•ˆí•´... ì§€ê¸ˆì€ ë§ˆìŒì´ ì¢€ ë³µì¡í•´ì„œ ì œëŒ€ë¡œ ë‹µë³€ì„ ì£¼ê¸° ì–´ë ¤ì›Œ. ì ì‹œ í›„ì— ë‹¤ì‹œ ì´ì•¼ê¸°í•´ì¤„ ìˆ˜ ìˆì„ê¹Œ..?"
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ëŠ” ì €ì¥
            try:
                user_msg = ChatMessage(
                    session_id=session_id,
                    sender_type="user",
                    content=user_message
                )
                self.db.add(user_msg)
                self.db.commit()
            except:
                pass
            
            return error_response
    
    def _generate_common_response(self, messages: list, user_message: str, **context) -> Tuple[str, Dict[str, int]]:
        """1ë‹¨ê³„: ê³µí†µ ê·œì¹™ìœ¼ë¡œ ê¸°ë³¸ ë‹µë³€ ìƒì„±"""
        try:
            # ê³µí†µ ê·œì¹™ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
            common_rules = self.chained_prompt_manager.load_common_rules()
            
            # LLMì— ë³´ë‚¼ ë©”ì‹œì§€ êµ¬ì„±
            llm_messages = []
            llm_messages.append(SystemMessage(content=f"""# ê±°ë¶ì´ìƒë‹´ì†Œ AI ìƒë‹´ì‚¬ - ê³µí†µ ë‹µë³€ ìƒì„±

{common_rules}

ìœ„ ê·œì¹™ì— ë”°ë¼ ì‚¬ìš©ìì˜ ë©”ì‹œì§€ì— ëŒ€í•´ ê¸°ë³¸ì ì´ê³  ì¤‘ë¦½ì ì¸ ìƒë‹´ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
ì´ ë‹µë³€ì€ ë‚˜ì¤‘ì— ê° í˜ë¥´ì†Œë‚˜ì˜ íŠ¹ì„±ì— ë§ê²Œ ë³€í™˜ë  ì˜ˆì •ì…ë‹ˆë‹¤."""))
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 8ê°œë§Œ)
            for msg in messages[-8:]:
                if msg.sender_type == "user":
                    llm_messages.append(HumanMessage(content=msg.content))
                else:
                    llm_messages.append(AIMessage(content=msg.content))
            
            # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            llm_messages.append(HumanMessage(content=user_message))
            
            # OpenAI API í˜¸ì¶œ
            response = self.llm.invoke(llm_messages)
            common_response = response.content
            
            # í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°
            tokens = self._calculate_tokens(llm_messages, common_response, response)
            
            return common_response, tokens
            
        except Exception as e:
            print(f"ê³µí†µ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            fallback_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì§€ê¸ˆ ë‹µë³€ì„ ìƒì„±í•˜ëŠ”ë° ì–´ë ¤ì›€ì´ ìˆì–´ìš”. ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ì£¼ì‹œê² ì–´ìš”?"
            return fallback_response, {'input': 0, 'output': 0}
    
    def _transform_to_persona(self, common_response: str, persona_type: str, user_message: str, **context) -> Tuple[str, Dict[str, int]]:
        """2ë‹¨ê³„: ê³µí†µ ë‹µë³€ì„ í˜ë¥´ì†Œë‚˜ íŠ¹ì„±ì— ë§ê²Œ ë³€í™˜"""
        try:
            # í˜ë¥´ì†Œë‚˜ ë§¤í•‘
            persona_mapping = {
                "ë‚´ë©´í˜•": "nemyeon",
                "ì¶”ì§„í˜•": "chujin", 
                "ê´€ê³„í˜•": "gwangye",
                "ì•ˆì •í˜•": "anjeong",
                "ì¾Œë½í˜•": "querock"
            }
            
            persona_key = persona_mapping.get(persona_type, "nemyeon")
            persona_prompt = self.chained_prompt_manager.load_persona_prompt(persona_key)
            
            # ë³€í™˜ìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            transform_prompt = f"""# í˜ë¥´ì†Œë‚˜ ë³€í™˜ ì‹œìŠ¤í…œ

ë‹¤ìŒì€ ë‹¹ì‹ ì˜ í˜ë¥´ì†Œë‚˜ íŠ¹ì„±ì…ë‹ˆë‹¤:

{persona_prompt}

## ë³€í™˜ ì‘ì—…
ìœ„ì˜ í˜ë¥´ì†Œë‚˜ íŠ¹ì„±ì— ë§ê²Œ ë‹¤ìŒ ê¸°ë³¸ ë‹µë³€ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë³€í™˜í•´ì£¼ì„¸ìš”:

**ê¸°ë³¸ ë‹µë³€:**
{common_response}

**ì‚¬ìš©ì ë©”ì‹œì§€:**
{user_message}

í˜ë¥´ì†Œë‚˜ì˜ ë§íˆ¬, ì„±ê²©, ìƒë‹´ ìŠ¤íƒ€ì¼ì„ ë°˜ì˜í•˜ì—¬ ë‹µë³€ì„ ë³€í™˜í•´ì£¼ì„¸ìš”. 
ë‹µë³€ì˜ í•µì‹¬ ë‚´ìš©ê³¼ ì˜ë„ëŠ” ìœ ì§€í•˜ë˜, í‘œí˜„ ë°©ì‹ì„ í˜ë¥´ì†Œë‚˜ì— ë§ê²Œ ì¡°ì •í•´ì£¼ì„¸ìš”."""
            
            llm_messages = [
                SystemMessage(content=transform_prompt),
                HumanMessage(content="ìœ„ ì§€ì¹¨ì— ë”°ë¼ ë‹µë³€ì„ ë³€í™˜í•´ì£¼ì„¸ìš”.")
            ]
            
            # OpenAI API í˜¸ì¶œ
            response = self.llm.invoke(llm_messages)
            persona_response = response.content
            
            # í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°
            tokens = self._calculate_tokens(llm_messages, persona_response, response)
            
            return persona_response, tokens
            
        except Exception as e:
            print(f"í˜ë¥´ì†Œë‚˜ ë³€í™˜ ì˜¤ë¥˜: {e}")
            # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê³µí†µ ë‹µë³€ ë°˜í™˜
            return common_response, {'input': 0, 'output': 0}
    
    def _calculate_tokens(self, llm_messages: list, response_content: str, response_obj) -> Dict[str, int]:
        """í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°"""
        if hasattr(response_obj, 'response_metadata') and 'token_usage' in response_obj.response_metadata:
            token_usage = response_obj.response_metadata['token_usage']
            return {
                'input': token_usage.get('prompt_tokens', 0),
                'output': token_usage.get('completion_tokens', 0)
            }
        else:
            # ëŒ€ëµì ì¸ í† í° ê³„ì‚° (1í† í° â‰ˆ 4ê¸€ì)
            all_text = " ".join([msg.content for msg in llm_messages if hasattr(msg, 'content')])
            estimated_input = len(all_text) // 4
            estimated_output = len(response_content) // 4
            return {
                'input': estimated_input,
                'output': estimated_output
            }

    def get_initial_greeting(self, persona_type: str = "ë‚´ë©´í˜•", user_analysis_result: dict = None) -> str:
        """í˜ë¥´ì†Œë‚˜ë³„ ì´ˆê¸° ì¸ì‚¬ ë©”ì‹œì§€ ë°˜í™˜ (ê·¸ë¦¼ ë¶„ì„ ê²°ê³¼ ë°˜ì˜)"""
        # ê¸°ë³¸ ì¸ì‚¬ë§
        base_greetings = {
            "ë‚´ë©´í˜•": "ì•ˆë…•... ë‚˜ëŠ” ë‚´ë©´ì´ì•¼. ë§ë³´ë‹¤ ëŠë‚Œì´ ë¨¼ì €ì¼ ë•Œ, ì¡°ìš©í•œ ë§ˆìŒìœ¼ë¡œ í•¨ê»˜ ìˆì„ ìˆ˜ ìˆë‹¤ë©´ ì¢‹ê² ì–´.",
            "ì¶”ì§„í˜•": "ì €ëŠ” ë‹¹ì‹ ì˜ ê³ ë¯¼ì„ í•¨ê»˜ í•´ê²°í•´ê°ˆ ì¶”ì§„ì´ì—ìš”. ì§€ê¸ˆë¶€í„° ê°€ì¥ ì¤‘ìš”í•œ ì–˜ê¸°ë¥¼ í•´ë³¼ê¹Œìš”?",
            "ê´€ê³„í˜•": "ì €ëŠ” ë‹¹ì‹ ì˜ ê³ ë¯¼ì„ í•¨ê»˜ í•´ê²°í•´ê°ˆ ê´€ê³„ì´ì—ìš”. ì§€ê¸ˆë¶€í„° ë§ˆìŒ ì† ê³ ë¯¼ì„ ì–˜ê¸°í•´ë³¼ê¹Œìš”?",
            "ì•ˆì •í˜•": "ì €ëŠ” ë‹¹ì‹ ì„ ì•ˆì •ì‹œì¼œë“œë¦´ ì•ˆì •ì´ì—ìš”. ì§€ê¸ˆë¶€í„° ë§ˆìŒ ì† ê³ ë¯¼ì„ ì–˜ê¸°í•´ë³¼ê¹Œìš”?",
            "ì¾Œë½í˜•": "ì €ëŠ” ë‹¹ì‹ ì˜ ê³ ë¯¼ì„ í•¨ê»˜ í•´ê²°í•´ê°ˆ ì¾Œë½ì´ì—ìš”. ì§€ê¸ˆë¶€í„° ì¬ë°ŒëŠ” ì–˜ê¸°ë¥¼ í•´ë³¼ê¹Œìš”?"
        }
        
        base_greeting = base_greetings.get(persona_type, base_greetings["ë‚´ë©´í˜•"])
        
        # ê·¸ë¦¼ ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë°˜ì˜
        if user_analysis_result and 'keyword_personality_analysis' in user_analysis_result:
            analysis_type = user_analysis_result['keyword_personality_analysis']['predicted_personality']
            print(f"ê·¸ë¦¼ ë¶„ì„ ê²°ê³¼ ë°˜ì˜: {analysis_type}")
            
            # ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ì¶”ê°€ ë©”ì‹œì§€
            analysis_additions = {
                "ì¶”ì§„í˜•": " ê·¸ë¦¼ì—ì„œ ë³´ë‹ˆ ë‹¹ì‹ ì€ ëª©í‘œë¥¼ í–¥í•´ ë‚˜ì•„ê°€ëŠ” í˜ì´ ìˆì–´ ë³´ì´ë„¤ìš”.",
                "ê´€ê³„í˜•": " ê·¸ë¦¼ì—ì„œ ë³´ë‹ˆ ë‹¹ì‹ ì€ ì‚¬ëŒë“¤ê³¼ì˜ ê´€ê³„ë¥¼ ì†Œì¤‘íˆ ì—¬ê¸°ëŠ” ê²ƒ ê°™ì•„ìš”.",
                "ì•ˆì •í˜•": " ê·¸ë¦¼ì—ì„œ ë³´ë‹ˆ ë‹¹ì‹ ì€ ì•ˆì •ì ì´ê³  ì°¨ë¶„í•œ ì„±í–¥ì„ ê°€ì§€ê³  ê³„ì‹œë„¤ìš”.",
                "ë‚´ë©´í˜•": " ê·¸ë¦¼ì—ì„œ ë³´ë‹ˆ ë‹¹ì‹ ë§Œì˜ ê¹Šì€ ë‚´ë©´ì„¸ê³„ê°€ ëŠê»´ì ¸ìš”.",
                "ì¾Œë½í˜•": " ê·¸ë¦¼ì—ì„œ ë³´ë‹ˆ ë‹¹ì‹ ì€ ì¦ê±°ì›€ê³¼ í™œë ¥ì´ ë„˜ì¹˜ëŠ” ë¶„ì´ë„¤ìš”."
            }
            
            if analysis_type in analysis_additions:
                base_greeting += analysis_additions[analysis_type]
        
        return base_greeting
    
    def get_available_personas(self) -> list:
        """ì‚¬ìš© ê°€ëŠ¥í•œ í˜ë¥´ì†Œë‚˜ ëª©ë¡ ë°˜í™˜"""
        return self.prompt_manager.get_available_personas()
"""
ê±°ë¶ì´ìƒë‹´ì†Œ HTP ì‹¬ë¦¬ê²€ì‚¬ íŒŒì´í”„ë¼ì¸ API

ì´ ëª¨ë“ˆì€ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì´ë¯¸ì§€ ë¶„ì„ ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
TestPage.tsxì—ì„œ ë²„íŠ¼ í´ë¦­ ì‹œ í˜¸ì¶œë˜ëŠ” í†µí•© íŒŒì´í”„ë¼ì¸ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.
"""

from fastapi import APIRouter, HTTPException, Depends, UploadFile, File, Form, BackgroundTasks
from fastapi.responses import JSONResponse
from sqlalchemy.orm import Session
from typing import Optional, Dict, Any
import os
import uuid
import json
import asyncio
from datetime import datetime
from pathlib import Path

# ë‚´ë¶€ ëª¨ë“ˆ
from ..database import get_db
from ..models.test import DrawingTest, DrawingTestResult
from ..models.user import UserInformation
from ..schemas.test import DrawingTestCreate, DrawingTestResultCreate
from .auth import get_current_user

# HTP íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆ (ì ˆëŒ€ ê²½ë¡œë¡œ import)
import sys
project_root = Path(__file__).parent.parent.parent
pipeline_module_path = project_root / 'llm' / 'model'
sys.path.insert(0, str(pipeline_module_path))

try:
    from main import HTPAnalysisPipeline, PipelineStatus, PipelineResult
    PIPELINE_IMPORT_ERROR = None
    print("âœ… HTP íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆ import ì„±ê³µ")
except Exception as e:
    import sys
    error_msg = str(e)
    print(f"âŒ HTP íŒŒì´í”„ë¼ì¸ import ì‹¤íŒ¨: {e}", file=sys.stderr)
    
    if "numpy.dtype size changed" in error_msg:
        print(f"ğŸ’¡ numpy/pandas ë²„ì „ ì¶©ëŒ í•´ê²°ë°©ë²•:", file=sys.stderr)
        print(f"   conda install -c conda-forge numpy pandas --force-reinstall", file=sys.stderr)
        print(f"   ë˜ëŠ” pip uninstall numpy pandas -y && pip install numpy pandas", file=sys.stderr)
    else:
        print(f"ğŸ’¡ ì¼ë°˜ì ì¸ í•´ê²°ë°©ë²•:", file=sys.stderr)
        print(f"   pip install pandas transformers ultralytics torch opencv-python scikit-learn", file=sys.stderr)
    
    HTPAnalysisPipeline = None
    PipelineStatus = None
    PipelineResult = None
    PIPELINE_IMPORT_ERROR = error_msg

router = APIRouter()

# ì „ì—­ íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤
pipeline_instance= None

def get_pipeline():
    """íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    global pipeline_instance
    if pipeline_instance is None:
        if HTPAnalysisPipeline is None:
            missing_packages = ["pandas", "transformers", "ultralytics", "torch", "opencv-python", "scikit-learn"]
            raise HTTPException(
                status_code=503,  # Service Unavailable
                detail={
                    "error": "HTP ë¶„ì„ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "reason": "í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                    "missing_packages": missing_packages,
                    "install_command": f"pip install {' '.join(missing_packages)}",
                    "status": "service_unavailable"
                }
            )
        pipeline_instance = HTPAnalysisPipeline()
    return pipeline_instance


@router.post("/analyze-image")
async def analyze_drawing_image(
    background_tasks: BackgroundTasks,
    file: Optional[UploadFile] = File(None),
    image: Optional[UploadFile] = File(None),
    description: Optional[str] = Form(None),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """
    ê·¸ë¦¼ ì´ë¯¸ì§€ ë¶„ì„ API
    
    TestPage.tsxì˜ 'ë¶„ì„ ì‹œì‘í•˜ê¸°' ë²„íŠ¼ì—ì„œ í˜¸ì¶œë©ë‹ˆë‹¤.
    ì—…ë¡œë“œëœ ì´ë¯¸ì§€ë¥¼ HTP ì‹¬ë¦¬ê²€ì‚¬ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        file: ì—…ë¡œë“œëœ ì´ë¯¸ì§€ íŒŒì¼
        description: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ê·¸ë¦¼ ì„¤ëª… (ì„ íƒì‚¬í•­)
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        current_user: í˜„ì¬ ë¡œê·¸ì¸í•œ ì‚¬ìš©ì
        
    Returns:
        JSON: ë¶„ì„ ì‘ì—… ì‹œì‘ ì‘ë‹µ ë° ì‘ì—… ID
    """
    # file ë˜ëŠ” image ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš© (í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ì„±)
    upload_file = file or image
    
    print(f"ğŸ” API ì—”ë“œí¬ì¸íŠ¸ ì§„ì… - í•¨ìˆ˜ ì‹œì‘")
    print(f"ğŸ“‹ ìš”ì²­ íŒŒë¼ë¯¸í„° ì •ë³´:")
    print(f"  - file: {file}")
    print(f"  - image: {image}")
    print(f"  - upload_file: {upload_file}")
    print(f"  - filename: {getattr(upload_file, 'filename', 'N/A') if upload_file else 'N/A'}")
    print(f"  - content_type: {getattr(upload_file, 'content_type', 'N/A') if upload_file else 'N/A'}")
    print(f"  - description: {description}")
    print(f"  - current_user: {current_user}")
    
    try:
        if not upload_file:
            print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•ŠìŒ")
            raise HTTPException(
                status_code=422,
                detail="ì´ë¯¸ì§€ íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'file' ë˜ëŠ” 'image' í•„ë“œì— íŒŒì¼ì„ ì²¨ë¶€í•´ì£¼ì„¸ìš”."
            )
        
        print(f"ğŸš€ ì´ë¯¸ì§€ ë¶„ì„ ìš”ì²­ ì‹œì‘ - ì‚¬ìš©ì: {current_user['user_id']}")
        print(f"ğŸ“ ì´ë¯¸ì§€ íŒŒì¼: {upload_file.filename}, í¬ê¸°: {upload_file.size if upload_file.size else 'unknown'}, íƒ€ì…: {upload_file.content_type}")
        print(f"ğŸ“ ì„¤ëª…: {description}")
        
        if not upload_file.filename:
            print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: íŒŒì¼ëª…ì´ ì—†ìŒ")
            raise HTTPException(
                status_code=422,
                detail="ì´ë¯¸ì§€ íŒŒì¼ëª…ì´ ì—†ìŠµë‹ˆë‹¤."
            )
        
        # 1. íŒŒì¼ ê²€ì¦

        if not upload_file.content_type or not upload_file.content_type.startswith('image/'):
            print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: ì˜ëª»ëœ content-type: {upload_file.content_type}")
            raise HTTPException(
                status_code=422,
                detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
            )
        
        # 2. ê³ ìœ  íŒŒì¼ëª… ìƒì„±
        file_extension = Path(upload_file.filename).suffix.lower()
        if file_extension not in ['.jpg', '.jpeg', '.png', '.bmp', '.gif']:
            print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: ì§€ì›í•˜ì§€ ì•ŠëŠ” í™•ì¥ì: {file_extension}")
            raise HTTPException(
                status_code=422,
                detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹ì…ë‹ˆë‹¤. (.jpg, .jpeg, .png, .bmp, .gif ì§€ì›)"
            )
        
        unique_id = str(uuid.uuid4())
        image_filename = f"{unique_id}{file_extension}"
        
        # 3. ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ ì„¤ì • (backend/result/images)
        backend_root = Path(__file__).parent.parent.parent
        upload_dir = backend_root / "result" / "images"
        upload_dir.mkdir(parents=True, exist_ok=True)
        
        # 4. íŒŒì¼ ì €ì¥ (JPGë¡œ í†µì¼)
        image_path = upload_dir / f"{unique_id}.jpg"
        
        # 5. íŒŒì´í”„ë¼ì¸ìš© ë””ë ‰í† ë¦¬ì—ë„ ë³µì‚¬ (ê¸°ì¡´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ í˜¸í™˜ì„±)
        pipeline = get_pipeline()
        pipeline_upload_dir = pipeline.config.test_img_dir
        pipeline_upload_dir.mkdir(parents=True, exist_ok=True)
        pipeline_image_path = pipeline_upload_dir / f"{unique_id}.jpg"
        
        # ì´ë¯¸ì§€ë¥¼ JPG í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
        import PIL.Image as PILImage
        import io
        
        # ì—…ë¡œë“œëœ íŒŒì¼ì„ PIL Imageë¡œ ë¡œë“œ
        image_data = await upload_file.read()
        pil_image = PILImage.open(io.BytesIO(image_data))
        
        # RGB ëª¨ë“œë¡œ ë³€í™˜ (RGBA ë“± ë‹¤ë¥¸ ëª¨ë“œ ì²˜ë¦¬)
        if pil_image.mode != 'RGB':
            pil_image = pil_image.convert('RGB')
        
        # JPGë¡œ ì €ì¥ (backend/result/images)
        pil_image.save(image_path, 'JPEG', quality=95)
        
        # íŒŒì´í”„ë¼ì¸ìš© ë””ë ‰í† ë¦¬ì—ë„ ì €ì¥
        pil_image.save(pipeline_image_path, 'JPEG', quality=95)
        
        # 6. ë°ì´í„°ë² ì´ìŠ¤ì— í…ŒìŠ¤íŠ¸ ë ˆì½”ë“œ ìƒì„±
        drawing_test = DrawingTest(
            user_id=current_user["user_id"],
            image_url=f"result/images/{unique_id}.jpg",  # backend/result/images ê²½ë¡œ
            submitted_at=datetime.now()
        )
        
        db.add(drawing_test)
        db.commit()
        db.refresh(drawing_test)
        
        # 7. ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¶„ì„ ì‹¤í–‰
        background_tasks.add_task(
            run_analysis_pipeline,
            unique_id,
            drawing_test.test_id,
            description
        )
        
        return JSONResponse(
            status_code=202,  # Accepted
            content={
                "message": "ì´ë¯¸ì§€ ë¶„ì„ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "test_id": drawing_test.test_id,
                "task_id": unique_id,
                "status": "processing",
                "estimated_time": "2-3ë¶„ ì†Œìš” ì˜ˆìƒ"
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ì´ë¯¸ì§€ ë¶„ì„ ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


def run_analysis_pipeline(
    unique_id: str,
    test_id: int,
    description: Optional[str]
):
    """
    ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” HTP ë¶„ì„ íŒŒì´í”„ë¼ì¸
    
    Args:
        unique_id: ê³ ìœ  ì´ë¯¸ì§€ ID
        test_id: ë°ì´í„°ë² ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ID
        description: ì‚¬ìš©ì ì„¤ëª…
    """
    # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ìš© ìƒˆ DB ì„¸ì…˜ ìƒì„± (HTTP ìš”ì²­ ì„¸ì…˜ê³¼ ë…ë¦½ì )
    from ..database import SessionLocal
    db = SessionLocal()
    
    try:
        print(f"ğŸš€ ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ì‹œì‘: {unique_id}")
        
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        pipeline = get_pipeline()
        result: PipelineResult = pipeline.analyze_image(unique_id)
        
        print(f"ğŸ“Š íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ: {result.status}")
        
        # ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ë™ê¸° í•¨ìˆ˜ë¡œ ë³€ê²½)
        save_analysis_result_sync(result, test_id, description, db)
        
        print(f"âœ… ë¶„ì„ ì™„ë£Œ ë° ì €ì¥: {unique_id}")
        
    except Exception as e:
        print(f"âŒ ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë°ì´í„°ë² ì´ìŠ¤ì— ì˜¤ë¥˜ ìƒíƒœ ì €ì¥
        try:
            pipeline = get_pipeline()
            pipeline.logger.error(f"ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        except:
            print(f"íŒŒì´í”„ë¼ì¸ ë¡œê±° ì‚¬ìš© ë¶ˆê°€: {str(e)}")
        
        # ë¹ˆ ê²°ê³¼ë¡œ ì˜¤ë¥˜ ìƒíƒœ ì €ì¥
        try:
            error_result = DrawingTestResult(
                test_id=test_id,
                friends_type=None,
                summary_text=f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                created_at=datetime.now()
            )
            
            db.add(error_result)
            db.commit()
            print(f"ì˜¤ë¥˜ ìƒíƒœ ì €ì¥ ì™„ë£Œ: {test_id}")
        except Exception as db_error:
            print(f"ì˜¤ë¥˜ ìƒíƒœ ì €ì¥ ì‹¤íŒ¨: {db_error}")
    finally:
        db.close()


def save_analysis_result_sync(
    result: Any,  # PipelineResultê°€ Noneì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ Any ì‚¬ìš©
    test_id: int,
    description: Optional[str],
    db: Session
):
    """
    ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ë™ê¸° ë²„ì „)
    
    Args:
        result: íŒŒì´í”„ë¼ì¸ ë¶„ì„ ê²°ê³¼
        test_id: ë°ì´í„°ë² ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ID
        description: ì‚¬ìš©ì ì„¤ëª…
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
    """
    try:
        # íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        pipeline = get_pipeline()
        
        # ì„±ê²© ìœ í˜•ì„ friends í…Œì´ë¸”ì˜ IDë¡œ ë§¤í•‘
        personality_mapping = {
            "ì¶”ì§„í˜•": 1,  # ì¶”ì§„ì´
            "ë‚´ë©´í˜•": 2,  # ë‚´ë©´ì´  
            "ê´€ê³„í˜•": 3,  # ê´€ê³„ì´
            "ì¾Œë½í˜•": 4,  # ì¾Œë½ì´
            "ì•ˆì •í˜•": 5   # ì•ˆì •ì´
        }
        
        friends_type_id = None
        summary_text = "ë¶„ì„ì„ ì™„ë£Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        if (PipelineStatus is not None and 
            hasattr(result, 'status') and 
            result.status == PipelineStatus.SUCCESS and 
            hasattr(result, 'personality_type') and 
            result.personality_type):
            friends_type_id = personality_mapping.get(result.personality_type)
            
            # result íŒŒì¼ì—ì„œ í™•ë¥  ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            result_file_path = pipeline.config.detection_results_dir / "results" / f"result_{result.image_base}.json"
            probabilities = None
            if result_file_path.exists():
                try:
                    with open(result_file_path, 'r', encoding='utf-8') as f:
                        result_data = json.load(f)
                        personality_analysis = result_data.get('personality_analysis', {})
                        probabilities = personality_analysis.get('probabilities', {})
                except Exception as e:
                    pipeline.logger.warning(f"result íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            
            # ìƒì„¸ ë¶„ì„ ê²°ê³¼ êµ¬ì„±
            summary_parts = []
            
            if result.psychological_analysis:
                analysis = result.psychological_analysis
                summary_parts.append(f"ğŸ¯ ì„±ê²© ìœ í˜•: {result.personality_type}")
                summary_parts.append(f"ğŸ“Š ì‹ ë¢°ë„: {result.confidence_score:.1%}")
                summary_parts.append("")
                
                # í™•ë¥  ì •ë³´ ì¶”ê°€
                if probabilities:
                    summary_parts.append("ğŸ“ˆ ìœ í˜•ë³„ í™•ë¥  ë¶„ì„:")
                    for persona_type, prob in sorted(probabilities.items(), key=lambda x: -x[1]):
                        summary_parts.append(f"â€¢ {persona_type}: {prob:.1f}%")
                    summary_parts.append("")
                
                if analysis.get('result_text'):
                    summary_parts.append("ğŸ“‹ ì‹¬ë¦¬ ë¶„ì„ ìš”ì•½:")
                    summary_parts.append(analysis['result_text'])
                    summary_parts.append("")
                
                if analysis.get('items') and isinstance(analysis['items'], list):
                    summary_parts.append("ğŸ” ì„¸ë¶€ ë¶„ì„ ìš”ì†Œ:")
                    for item in analysis['items'][:5]:  # ìµœëŒ€ 5ê°œ í•­ëª©ë§Œ í‘œì‹œ
                        if isinstance(item, dict):
                            element = item.get('element', 'N/A')
                            condition = item.get('condition', 'N/A')
                            keywords = item.get('keywords', [])
                            if isinstance(keywords, list) and keywords:
                                keyword_str = ', '.join(keywords[:3])  # ìµœëŒ€ 3ê°œ í‚¤ì›Œë“œ
                                summary_parts.append(f"â€¢ {element}: {condition} ({keyword_str})")
            
            if description:
                summary_parts.append("")
                summary_parts.append("ğŸ’­ ì‚¬ìš©ì ì„¤ëª…:")
                summary_parts.append(description)
            
            summary_text = "\n".join(summary_parts)
            
        elif result.error_message:
            summary_text = f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {result.error_message}"
        
        # ê²°ê³¼ ì €ì¥ (upsert íŒ¨í„´)
        existing_result = db.query(DrawingTestResult).filter(
            DrawingTestResult.test_id == test_id
        ).first()
        
        if existing_result:
            # ê¸°ì¡´ ê²°ê³¼ ì—…ë°ì´íŠ¸
            existing_result.friends_type = friends_type_id
            existing_result.summary_text = summary_text
            existing_result.created_at = datetime.now()
        else:
            # ìƒˆ ê²°ê³¼ ìƒì„±
            test_result = DrawingTestResult(
                test_id=test_id,
                friends_type=friends_type_id,
                summary_text=summary_text,
                created_at=datetime.now()
            )
            db.add(test_result)
        
        db.commit()
        
    except Exception as e:
        db.rollback()
        try:
            pipeline.logger.error(f"ë¶„ì„ ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
        except:
            print(f"ë¶„ì„ ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
        raise


@router.get("/analysis-status/{test_id}")
async def get_analysis_status(
    test_id: int,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """
    ë¶„ì„ ìƒíƒœ ì¡°íšŒ API
    
    í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ë¶„ì„ ì§„í–‰ ìƒí™©ì„ í™•ì¸í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    Args:
        test_id: í…ŒìŠ¤íŠ¸ ID
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        current_user: í˜„ì¬ ì‚¬ìš©ì
        
    Returns:
        JSON: ë¶„ì„ ìƒíƒœ ì •ë³´
    """
    try:
        # í…ŒìŠ¤íŠ¸ ë ˆì½”ë“œ ì¡°íšŒ
        drawing_test = db.query(DrawingTest).filter(
            DrawingTest.test_id == test_id,
            DrawingTest.user_id == current_user["user_id"]
        ).first()
        
        if not drawing_test:
            raise HTTPException(
                status_code=404,
                detail="í•´ë‹¹ í…ŒìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        # ê²°ê³¼ ì¡°íšŒ
        test_result = db.query(DrawingTestResult).filter(
            DrawingTestResult.test_id == test_id
        ).first()
        
        if not test_result:
            # íŒŒì´í”„ë¼ì¸ ì§„í–‰ìƒí™© í™•ì¸
            pipeline = get_pipeline()
            
            # ì´ë¯¸ì§€ íŒŒì¼ëª… ì¶”ì¶œ (URLì—ì„œ íŒŒì¼ëª… ë¶€ë¶„ë§Œ)
            image_url = drawing_test.image_url  # "result/images/{unique_id}.jpg"
            if image_url:
                # "result/images/uuid.jpg" -> "uuid"
                import re
                match = re.search(r'result/images/(.+?)\.jpg', image_url)
                if match:
                    unique_id = match.group(1)
                    status_info = pipeline.get_analysis_status(unique_id)
                    
                    # ë‹¨ê³„ë³„ ì§„í–‰ìƒí™© êµ¬ì„±
                    detection_completed = status_info.get("detection_completed", False)
                    analysis_completed = status_info.get("analysis_completed", False)
                    classification_completed = status_info.get("classification_completed", False)
                    
                    steps = [
                        {
                            "name": "ê°ì²´ íƒì§€",
                            "description": "YOLOë¥¼ ì‚¬ìš©í•œ ê·¸ë¦¼ ìš”ì†Œ ê²€ì¶œ",
                            "completed": detection_completed,
                            "current": not detection_completed
                        },
                        {
                            "name": "ì‹¬ë¦¬ ë¶„ì„", 
                            "description": "GPT-4ë¥¼ ì‚¬ìš©í•œ ì‹¬ë¦¬ìƒíƒœ ë¶„ì„",
                            "completed": analysis_completed,
                            "current": detection_completed and not analysis_completed
                        },
                        {
                            "name": "ì„±ê²© ë¶„ë¥˜",
                            "description": "KoBERTë¥¼ ì‚¬ìš©í•œ ì„±ê²©ìœ í˜• ë¶„ë¥˜", 
                            "completed": classification_completed,
                            "current": analysis_completed and not classification_completed
                        }
                    ]
                    
                    # í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ë‹¨ê³„ ì°¾ê¸°
                    current_step = next((i+1 for i, step in enumerate(steps) if step["current"]), 1)
                    completed_steps = sum(1 for step in steps if step["completed"])
                    
                    # ëª¨ë“  ë‹¨ê³„ê°€ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
                    if classification_completed:
                        # 3ë‹¨ê³„ ëª¨ë‘ ì™„ë£Œëœ ê²½ìš°, ìµœì¢… ê²°ê³¼ê°€ DBì— ì €ì¥ë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
                        print(f"ğŸ¯ ëª¨ë“  ë‹¨ê³„ ì™„ë£Œë¨ - ìµœì¢… ê²°ê³¼ ëŒ€ê¸° ì¤‘")
                        return JSONResponse(content={
                            "test_id": test_id,
                            "status": "processing",
                            "message": "ìµœì¢… ê²°ê³¼ ìƒì„± ì¤‘...",
                            "steps": steps,
                            "current_step": 3,
                            "completed_steps": 3,
                            "total_steps": 3,
                            "estimated_remaining": "ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”"
                        })
                    
                    return JSONResponse(content={
                        "test_id": test_id,
                        "status": "processing",
                        "message": f"ë‹¨ê³„ {current_step}/3 ì§„í–‰ ì¤‘...",
                        "steps": steps,
                        "current_step": current_step,
                        "completed_steps": completed_steps,
                        "total_steps": 3,
                        "estimated_remaining": f"{4-completed_steps}ë¶„ ì†Œìš” ì˜ˆìƒ"
                    })
            
            # ê¸°ë³¸ ì‘ë‹µ (íŒŒì¼ëª… ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ)
            return JSONResponse(content={
                "test_id": test_id,
                "status": "processing", 
                "message": "ë¶„ì„ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤...",
                "steps": [
                    {"name": "ê°ì²´ íƒì§€", "description": "ê·¸ë¦¼ ìš”ì†Œ ê²€ì¶œ ì¤‘", "completed": False, "current": True},
                    {"name": "ì‹¬ë¦¬ ë¶„ì„", "description": "ì‹¬ë¦¬ìƒíƒœ ë¶„ì„ ëŒ€ê¸° ì¤‘", "completed": False, "current": False},
                    {"name": "ì„±ê²© ë¶„ë¥˜", "description": "ì„±ê²©ìœ í˜• ë¶„ë¥˜ ëŒ€ê¸° ì¤‘", "completed": False, "current": False}
                ],
                "current_step": 1,
                "completed_steps": 0,
                "total_steps": 3,
                "estimated_remaining": "2-3ë¶„"
            })
        
        # result íŒŒì¼ì—ì„œ ì¶”ê°€ ë°ì´í„° ì½ê¸°
        pipeline = get_pipeline()
        image_url = drawing_test.image_url
        result_text = None
        predicted_personality = None
        probabilities = {}
        
        if image_url:
            import re
            match = re.search(r'result/images/(.+?)\.jpg', image_url)
            if match:
                unique_id = match.group(1)
                result_file_path = pipeline.config.detection_results_dir / "results" / f"result_{unique_id}.json"
                
                if result_file_path.exists():
                    try:
                        with open(result_file_path, 'r', encoding='utf-8') as f:
                            result_data = json.load(f)
                            result_text = result_data.get('result_text', '')
                            personality_analysis = result_data.get('personality_analysis', {})
                            predicted_personality = personality_analysis.get('predicted_personality', '')
                            probabilities = personality_analysis.get('probabilities', {})
                    except Exception as e:
                        pipeline.logger.warning(f"result íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")

        # ë¶„ì„ ì™„ë£Œ
        return JSONResponse(content={
            "test_id": test_id,
            "status": "completed",
            "message": "ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "steps": [
                {"name": "ê°ì²´ íƒì§€", "description": "YOLOë¥¼ ì‚¬ìš©í•œ ê·¸ë¦¼ ìš”ì†Œ ê²€ì¶œ", "completed": True, "current": False},
                {"name": "ì‹¬ë¦¬ ë¶„ì„", "description": "GPT-4ë¥¼ ì‚¬ìš©í•œ ì‹¬ë¦¬ìƒíƒœ ë¶„ì„", "completed": True, "current": False},
                {"name": "ì„±ê²© ë¶„ë¥˜", "description": "KoBERTë¥¼ ì‚¬ìš©í•œ ì„±ê²©ìœ í˜• ë¶„ë¥˜", "completed": True, "current": False}
            ],
            "current_step": 3,
            "completed_steps": 3,
            "total_steps": 3,
            "result": {
                "friends_type": test_result.friends_type,
                "summary_text": test_result.summary_text,
                "result_text": result_text,
                "predicted_personality": predicted_personality,
                "probabilities": probabilities,
                "created_at": test_result.created_at.isoformat() if test_result.created_at else None
            }
        })
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ë¶„ì„ ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get("/pipeline-health")
async def check_pipeline_health():
    """
    íŒŒì´í”„ë¼ì¸ ìƒíƒœ í™•ì¸ API
    
    ì‹œìŠ¤í…œ ê´€ë¦¬ìê°€ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ìš”ì†Œì˜ ìƒíƒœë¥¼ í™•ì¸í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    Returns:
        JSON: íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì •ë³´
    """
    try:
        # ê¸°ë³¸ ìƒíƒœ ì •ë³´
        status = {
            "pipeline_status": "unknown",
            "timestamp": datetime.now().isoformat(),
            "components": {
                "pipeline_import": False,
                "yolo_model": False,
                "openai_api": False,
                "kobert_model": False,
                "directories": False
            },
            "directories": {},
            "error_details": None
        }
        
        # íŒŒì´í”„ë¼ì¸ import í™•ì¸
        if HTPAnalysisPipeline is not None:
            status["components"]["pipeline_import"] = True
            
            try:
                pipeline = get_pipeline()
                
                status["directories"] = {
                    "test_images": str(pipeline.config.test_img_dir),
                    "detection_results": str(pipeline.config.detection_results_dir),
                    "rag_docs": str(pipeline.config.rag_dir)
                }
                
                # YOLO ëª¨ë¸ í™•ì¸
                yolo_path = pipeline.config.model_dir / pipeline.config.yolo_model_path
                status["components"]["yolo_model"] = yolo_path.exists()
                
                # OpenAI API í‚¤ í™•ì¸
                status["components"]["openai_api"] = bool(os.getenv('OPENAI_API_KEY'))
                
                # KoBERT ëª¨ë¸ í™•ì¸
                kobert_path = pipeline.config.model_dir / pipeline.config.kobert_model_path
                status["components"]["kobert_model"] = kobert_path.exists()
                
                # ë””ë ‰í† ë¦¬ í™•ì¸
                required_dirs = [
                    pipeline.config.test_img_dir,
                    pipeline.config.detection_results_dir,
                    pipeline.config.rag_dir
                ]
                status["components"]["directories"] = all(d.exists() for d in required_dirs)
                
            except Exception as pipeline_error:
                status["error_details"] = f"Pipeline initialization failed: {str(pipeline_error)}"
        else:
            if PIPELINE_IMPORT_ERROR and "numpy.dtype size changed" in PIPELINE_IMPORT_ERROR:
                status["error_details"] = {
                    "message": "numpy/pandas ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ",
                    "error": PIPELINE_IMPORT_ERROR,
                    "solution": "conda install -c conda-forge numpy pandas --force-reinstall",
                    "alternative": "pip uninstall numpy pandas -y && pip install numpy pandas",
                    "help": "numpyì™€ pandasì˜ ë²„ì „ì´ í˜¸í™˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¬ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
                }
            else:
                missing_packages = ["pandas", "transformers", "ultralytics", "torch", "opencv-python", "scikit-learn"]
                status["error_details"] = {
                    "message": "HTPAnalysisPipeline could not be imported",
                    "error": PIPELINE_IMPORT_ERROR or "Unknown import error",
                    "missing_packages": missing_packages,
                    "install_command": f"pip install {' '.join(missing_packages)}",
                    "help": "HTP ë¶„ì„ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ ìœ„ íŒ¨í‚¤ì§€ë“¤ì„ ì„¤ì¹˜í•´ì£¼ì„¸ìš”."
                }
        
        # ì „ì²´ ìƒíƒœ íŒë‹¨
        if status["components"]["pipeline_import"]:
            all_healthy = all(status["components"].values())
            status["pipeline_status"] = "healthy" if all_healthy else "degraded"
        else:
            status["pipeline_status"] = "unavailable"
        
        return JSONResponse(content=status)
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={
                "pipeline_status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "error_details": "Health check failed completely"
            }
        )
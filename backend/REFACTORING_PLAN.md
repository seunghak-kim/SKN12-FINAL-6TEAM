# 코드 리팩토링 및 최적화 계획

## 1. 리팩토링 계획 (Refactoring Plan)

현재 코드는 기능적으로 잘 동작하지만, 유지보수성과 확장성을 위해 다음과 같은 리팩토링을 제안합니다.

### 1.1 구조 개선 (Architecture)
- **Service Layer 도입**: `backend/app/api/pipeline.py`에 있는 비즈니스 로직을 `backend/app/services/analysis_service.py`로 분리하여 API 계층과 비즈니스 로직을 분리합니다.
- **의존성 주입 (Dependency Injection)**: `get_pipeline()` 함수 대신 FastAPI의 `Depends`를 활용하여 서비스 인스턴스를 주입받도록 변경합니다.

### 1.2 코드 품질 (Code Quality)
- **하드코딩 제거**: 파일 경로, 모델 설정값 등을 `config.py` 또는 환경 변수로 관리합니다.
- **파일 기반 통신 제거**: 현재 단계별 상태 확인을 위해 파일 존재 여부(`exists()`)를 체크하는 방식을 메모리 내 객체 전달이나 DB 상태 값 확인으로 변경하여 I/O 오버헤드를 줄입니다.
- **동기/비동기 명확화**: `run_analysis_pipeline` 내부의 블로킹 연산과 비동기 연산을 명확히 구분하고, 필요한 경우 `asyncio`를 적극 활용합니다.

## 2. 최적화 방안 (Optimization Strategies)

### 2.1 YOLO 최적화 (Object Detection)
현재 `320x320, quality=10`으로 이미지를 압축하여 YOLO에 전달하고 있습니다. 이는 속도에는 유리하지만 정확도에 영향을 줄 수 있습니다.

- **ONNX Runtime 도입**: PyTorch 모델을 ONNX로 변환하여 CPU 추론 속도를 2-3배 향상시킵니다.
- **이미지 전처리 최적화**: 
  - `quality=10`은 너무 낮은 품질일 수 있습니다. `quality=50` 정도로 높이더라도 해상도를 `416x416` 또는 `320x320`으로 유지하면 속도 저하는 미미하고 정확도는 향상될 수 있습니다.
  - 메모리 상에서 이미지를 처리하여 디스크 I/O를 줄입니다 (현재는 저장 후 다시 로드함).
- **모델 양자화 (Quantization)**: FP16 또는 INT8 양자화를 적용하여 모델 크기를 줄이고 추론 속도를 높입니다.

### 2.2 LLM 요약 최적화 (LLM Summarization)
GPT-4 Vision을 사용하는 단계에서 비용과 속도가 병목이 될 수 있습니다.

- **프롬프트 최적화 (Prompt Engineering)**:
  - 불필요한 서술을 줄이고 JSON 포맷으로 직접 응답받도록 프롬프트를 수정하여 파싱 비용을 줄입니다.
  - `Few-shot prompting`을 통해 원하는 출력 형식을 명확히 하여 재시도 횟수를 줄입니다.
- **캐싱 (Caching)**: 동일하거나 매우 유사한 이미지/요청에 대한 LLM 응답을 캐싱합니다.
- **하이브리드 접근**: 
  - 이미지 분석은 GPT-4 Vision이 담당하고, 텍스트 요약 및 성격 분류는 더 가볍고 빠른 모델(GPT-3.5-turbo 또는 로컬 LLM)이 담당하도록 파이프라인을 분리합니다.
- **스트리밍 (Streaming)**: 사용자 경험을 위해 분석 텍스트가 생성되는 대로 프론트엔드에 스트리밍하는 방식을 고려합니다 (현재 구조상 어려울 수 있으나 장기적으로 추천).

## 3. 실행 단계

1. **1단계**: `backend/app/services` 디렉토리 생성 및 로직 이관
2. **2단계**: YOLO 모델 ONNX 변환 및 `image_processor.py` 최적화
3. **3단계**: LLM 프롬프트 개선 및 구조화된 출력 적용

이 계획에 따라 진행하시겠습니까? 승인하시면 1단계부터 작업을 시작하겠습니다.
